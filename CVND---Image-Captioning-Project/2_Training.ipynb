{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computer Vision Nanodegree\n",
    "\n",
    "## Project: Image Captioning\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will train your CNN-RNN model.  \n",
    "\n",
    "You are welcome and encouraged to try out many different architectures and hyperparameters when searching for a good model.\n",
    "\n",
    "This does have the potential to make the project quite messy!  Before submitting your project, make sure that you clean up:\n",
    "- the code you write in this notebook.  The notebook should describe how to train a single CNN-RNN architecture, corresponding to your final choice of hyperparameters.  You should structure the notebook so that the reviewer can replicate your results by running the code in this notebook.  \n",
    "- the output of the code cell in **Step 2**.  The output should show the output obtained when training the model from scratch.\n",
    "\n",
    "This notebook **will be graded**.  \n",
    "\n",
    "Feel free to use the links below to navigate the notebook:\n",
    "- [Step 1](#step1): Training Setup\n",
    "- [Step 2](#step2): Train your Model\n",
    "- [Step 3](#step3): (Optional) Validate your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step1'></a>\n",
    "## Step 1: Training Setup\n",
    "\n",
    "In this step of the notebook, you will customize the training of your CNN-RNN model by specifying hyperparameters and setting other options that are important to the training procedure.  The values you set now will be used when training your model in **Step 2** below.\n",
    "\n",
    "You should only amend blocks of code that are preceded by a `TODO` statement.  **Any code blocks that are not preceded by a `TODO` statement should not be modified**.\n",
    "\n",
    "### Task #1\n",
    "\n",
    "Begin by setting the following variables:\n",
    "- `batch_size` - the batch size of each training batch.  It is the number of image-caption pairs used to amend the model weights in each training step. \n",
    "- `vocab_threshold` - the minimum word count threshold.  Note that a larger threshold will result in a smaller vocabulary, whereas a smaller threshold will include rarer words and result in a larger vocabulary.  \n",
    "- `vocab_from_file` - a Boolean that decides whether to load the vocabulary from file. \n",
    "- `embed_size` - the dimensionality of the image and word embeddings.  \n",
    "- `hidden_size` - the number of features in the hidden state of the RNN decoder.  \n",
    "- `num_epochs` - the number of epochs to train the model.  We recommend that you set `num_epochs=3`, but feel free to increase or decrease this number as you wish.  [This paper](https://arxiv.org/pdf/1502.03044.pdf) trained a captioning model on a single state-of-the-art GPU for 3 days, but you'll soon see that you can get reasonable results in a matter of a few hours!  (_But of course, if you want your model to compete with current research, you will have to train for much longer._)\n",
    "- `save_every` - determines how often to save the model weights.  We recommend that you set `save_every=1`, to save the model weights after each epoch.  This way, after the `i`th epoch, the encoder and decoder weights will be saved in the `models/` folder as `encoder-i.pkl` and `decoder-i.pkl`, respectively.\n",
    "- `print_every` - determines how often to print the batch loss to the Jupyter notebook while training.  Note that you **will not** observe a monotonic decrease in the loss function while training - this is perfectly fine and completely expected!  You are encouraged to keep this at its default value of `100` to avoid clogging the notebook, but feel free to change it.\n",
    "- `log_file` - the name of the text file containing - for every step - how the loss and perplexity evolved during training.\n",
    "\n",
    "If you're not sure where to begin to set some of the values above, you can peruse [this paper](https://arxiv.org/pdf/1502.03044.pdf) and [this paper](https://arxiv.org/pdf/1411.4555.pdf) for useful guidance!  **To avoid spending too long on this notebook**, you are encouraged to consult these suggested research papers to obtain a strong initial guess for which hyperparameters are likely to work best.  Then, train a single model, and proceed to the next notebook (**3_Inference.ipynb**).  If you are unhappy with your performance, you can return to this notebook to tweak the hyperparameters (and/or the architecture in **model.py**) and re-train your model.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "**Question:** Describe your CNN-RNN architecture in detail.  With this architecture in mind, how did you select the values of the variables in Task 1?  If you consulted a research paper detailing a successful implementation of an image captioning model, please provide the reference.\n",
    "\n",
    "**Answer:** I use resnet50 as encoder model to deal with the images imformation,and use the last fullly conntion as a out which embeded as the iupute feature in rnn decoder model.In decoder model firstly,I embed captions and images\n",
    "and then push them in to LSTM to calculate outs and new hidden layers,at last using a linear function to make output result as a lenth of vocabulary.\n",
    "\n",
    "\n",
    "### (Optional) Task #2\n",
    "\n",
    "Note that we have provided a recommended image transform `transform_train` for pre-processing the training images, but you are welcome (and encouraged!) to modify it as you wish.  When modifying this transform, keep in mind that:\n",
    "- the images in the dataset have varying heights and widths, and \n",
    "- if using a pre-trained model, you must perform the corresponding appropriate normalization.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "**Question:** How did you select the transform in `transform_train`?  If you left the transform at its provided value, why do you think that it is a good choice for your CNN architecture?\n",
    "\n",
    "**Answer:** Using transformation can make the model get better generalization results.First, smaller edge of image resized to 256,then get 224x224 crop from random location , horizontally flip image with probability=0.5 and normalize image for pre-trained model.So that operation can make the images that we trainning,can more get complex\n",
    "feature,and enhanced generalization capability.\n",
    "\n",
    "### Task #3\n",
    "\n",
    "Next, you will specify a Python list containing the learnable parameters of the model.  For instance, if you decide to make all weights in the decoder trainable, but only want to train the weights in the embedding layer of the encoder, then you should set `params` to something like:\n",
    "```\n",
    "params = list(decoder.parameters()) + list(encoder.embed.parameters()) \n",
    "```\n",
    "\n",
    "### Question 3\n",
    "\n",
    "**Question:** How did you select the trainable parameters of your architecture?  Why do you think this is a good choice?\n",
    "\n",
    "**Answer:** batch_size 10 maybe not perfect,In point of view the range 64 to 128 may be better.but my hardware can't do it.vocab_threshold 4 same as nook book 2.embed_size 256 conform to reality.For lstm hidden_size = 512     \n",
    "num_epochs my be 2 is ok,I see when epoch equals 2 the loos is steady.\n",
    "\n",
    "### Task #4\n",
    "\n",
    "Finally, you will select an [optimizer](http://pytorch.org/docs/master/optim.html#torch.optim.Optimizer).\n",
    "\n",
    "### Question 4\n",
    "\n",
    "**Question:** How did you select the optimizer used to train your model?\n",
    "\n",
    "**Answer:** I use Adam optimizer.1. simple implementation, efficient computation and less memory requirements.\n",
    "\n",
    "The updating of 2. parameters is not affected by the scaling transformation of gradient.3.the hyper parameters are well explanatory and usually do not need to be adjusted or require only minor fine-tuning.4.the updated step size can be restricted to a general range (initial learning rate).5. we can automatically achieve step annealing process (automatically adjusting learning rate).6.it is suitable for large-scale data and parameter scenarios.7. for unstable objective function8. it is suitable for gradient sparse or large gradient noise problems.lr=0.001 is ok.\n",
    "not too slow and get a better loos value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary successfully loaded from vocab.pkl file!\n",
      "loading annotations into memory...\n",
      "Done (t=0.56s)\n",
      "creating index...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 861/414113 [00:00<00:48, 8602.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index created!\n",
      "Obtaining caption lengths...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 414113/414113 [00:39<00:00, 10464.61it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "sys.path.append('/opt/cocoapi/PythonAPI')\n",
    "from pycocotools.coco import COCO\n",
    "from data_loader import get_loader\n",
    "from model import EncoderCNN, DecoderRNN\n",
    "import math\n",
    "\n",
    "\n",
    "## TODO #1: Select appropriate values for the Python variables below.\n",
    "batch_size = 10          # batch size\n",
    "vocab_threshold = 4        # minimum word count threshold\n",
    "vocab_from_file = True    # if True, load existing vocab file\n",
    "embed_size = 256           # dimensionality of image and word embeddings\n",
    "hidden_size = 512          # number of features in hidden state of the RNN decoder\n",
    "num_epochs = 3             # number of training epochs\n",
    "save_every = 1             # determines frequency of saving model weights\n",
    "print_every = 100          # determines window for printing average loss\n",
    "log_file = 'training_log.txt'       # name of file with saved training loss and perplexity\n",
    "\n",
    "# (Optional) TODO #2: Amend the image transform below.\n",
    "transform_train = transforms.Compose([ \n",
    "    transforms.Resize(256),                          # smaller edge of image resized to 256\n",
    "    transforms.RandomCrop(224),                      # get 224x224 crop from random location\n",
    "    transforms.RandomHorizontalFlip(),               # horizontally flip image with probability=0.5\n",
    "    transforms.ToTensor(),                           # convert the PIL Image to a tensor\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),      # normalize image for pre-trained model\n",
    "                         (0.229, 0.224, 0.225))])\n",
    "\n",
    "# Build data loader.\n",
    "data_loader = get_loader(transform=transform_train,\n",
    "                         mode='train',\n",
    "                         batch_size=batch_size,\n",
    "                         vocab_threshold=vocab_threshold,\n",
    "                         vocab_from_file=vocab_from_file)\n",
    "\n",
    "# The size of the vocabulary.\n",
    "vocab_size = len(data_loader.dataset.vocab)\n",
    "\n",
    "# Initialize the encoder and decoder. \n",
    "encoder = EncoderCNN(embed_size)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, vocab_size)\n",
    "\n",
    "# Move models to GPU if CUDA is available. \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "# Define the loss function. \n",
    "criterion = nn.CrossEntropyLoss().cuda() if torch.cuda.is_available() else nn.CrossEntropyLoss()\n",
    "\n",
    "# TODO #3: Specify the learnable parameters of the model.\n",
    "params = list(encoder.embed.parameters())+list(decoder.parameters()) \n",
    "\n",
    "# TODO #4: Define the optimizer.\n",
    "optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "\n",
    "# Set the total number of training steps per epoch.\n",
    "total_step = math.ceil(len(data_loader.dataset.caption_lengths) / data_loader.batch_sampler.batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step2'></a>\n",
    "## Step 2: Train your Model\n",
    "\n",
    "Once you have executed the code cell in **Step 1**, the training procedure below should run without issue.  \n",
    "\n",
    "It is completely fine to leave the code cell below as-is without modifications to train your model.  However, if you would like to modify the code used to train the model below, you must ensure that your changes are easily parsed by your reviewer.  In other words, make sure to provide appropriate comments to describe how your code works!  \n",
    "\n",
    "You may find it useful to load saved weights to resume training.  In that case, note the names of the files containing the encoder and decoder weights that you'd like to load (`encoder_file` and `decoder_file`).  Then you can load the weights by using the lines below:\n",
    "\n",
    "```python\n",
    "# Load pre-trained weights before resuming training.\n",
    "encoder.load_state_dict(torch.load(os.path.join('./models', encoder_file)))\n",
    "decoder.load_state_dict(torch.load(os.path.join('./models', decoder_file)))\n",
    "```\n",
    "\n",
    "While trying out parameters, make sure to take extensive notes and record the settings that you used in your various training runs.  In particular, you don't want to encounter a situation where you've trained a model for several hours but can't remember what settings you used :).\n",
    "\n",
    "### A Note on Tuning Hyperparameters\n",
    "\n",
    "To figure out how well your model is doing, you can look at how the training loss and perplexity evolve during training - and for the purposes of this project, you are encouraged to amend the hyperparameters based on this information.  \n",
    "\n",
    "However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models.  \n",
    "\n",
    "For this project, you need not worry about overfitting. **This project does not have strict requirements regarding the performance of your model**, and you just need to demonstrate that your model has learned **_something_** when you generate captions on the test data.  For now, we strongly encourage you to train your model for the suggested 3 epochs without worrying about performance; then, you should immediately transition to the next notebook in the sequence (**3_Inference.ipynb**) to see how your model performs on the test data.  If your model needs to be changed, you can come back to this notebook, amend hyperparameters (if necessary), and re-train the model.\n",
    "\n",
    "That said, if you would like to go above and beyond in this project, you can read about some approaches to minimizing overfitting in section 4.3.1 of [this paper](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7505636).  In the next (optional) step of this notebook, we provide some guidance for assessing the performance on the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [100/41412], Loss: 4.6511, Perplexity: 104.6973\n",
      "Epoch [1/3], Step [200/41412], Loss: 4.2013, Perplexity: 66.77222\n",
      "Epoch [1/3], Step [300/41412], Loss: 4.2426, Perplexity: 69.58703\n",
      "Epoch [1/3], Step [400/41412], Loss: 3.7829, Perplexity: 43.94512\n",
      "Epoch [1/3], Step [500/41412], Loss: 4.1807, Perplexity: 65.41229\n",
      "Epoch [1/3], Step [600/41412], Loss: 3.6789, Perplexity: 39.60313\n",
      "Epoch [1/3], Step [700/41412], Loss: 3.3697, Perplexity: 29.06929\n",
      "Epoch [1/3], Step [800/41412], Loss: 3.9022, Perplexity: 49.51246\n",
      "Epoch [1/3], Step [900/41412], Loss: 3.0217, Perplexity: 20.52669\n",
      "Epoch [1/3], Step [1000/41412], Loss: 3.3261, Perplexity: 27.8308\n",
      "Epoch [1/3], Step [1100/41412], Loss: 3.9995, Perplexity: 54.57105\n",
      "Epoch [1/3], Step [1200/41412], Loss: 3.5807, Perplexity: 35.8981\n",
      "Epoch [1/3], Step [1300/41412], Loss: 3.1228, Perplexity: 22.70963\n",
      "Epoch [1/3], Step [1400/41412], Loss: 3.2660, Perplexity: 26.2051\n",
      "Epoch [1/3], Step [1500/41412], Loss: 3.8195, Perplexity: 45.5802\n",
      "Epoch [1/3], Step [1600/41412], Loss: 3.6883, Perplexity: 39.97801\n",
      "Epoch [1/3], Step [1700/41412], Loss: 3.6450, Perplexity: 38.2827\n",
      "Epoch [1/3], Step [1800/41412], Loss: 3.3965, Perplexity: 29.8595\n",
      "Epoch [1/3], Step [1900/41412], Loss: 3.7078, Perplexity: 40.7632\n",
      "Epoch [1/3], Step [2000/41412], Loss: 3.3697, Perplexity: 29.0688\n",
      "Epoch [1/3], Step [2100/41412], Loss: 3.8025, Perplexity: 44.8136\n",
      "Epoch [1/3], Step [2200/41412], Loss: 3.1869, Perplexity: 24.2141\n",
      "Epoch [1/3], Step [2300/41412], Loss: 3.6641, Perplexity: 39.0202\n",
      "Epoch [1/3], Step [2400/41412], Loss: 3.4191, Perplexity: 30.5404\n",
      "Epoch [1/3], Step [2500/41412], Loss: 3.1903, Perplexity: 24.2964\n",
      "Epoch [1/3], Step [2600/41412], Loss: 2.6483, Perplexity: 14.1302\n",
      "Epoch [1/3], Step [2700/41412], Loss: 3.1418, Perplexity: 23.1464\n",
      "Epoch [1/3], Step [2800/41412], Loss: 3.4169, Perplexity: 30.4739\n",
      "Epoch [1/3], Step [2900/41412], Loss: 2.6030, Perplexity: 13.5044\n",
      "Epoch [1/3], Step [3000/41412], Loss: 3.8034, Perplexity: 44.8520\n",
      "Epoch [1/3], Step [3100/41412], Loss: 2.9620, Perplexity: 19.3367\n",
      "Epoch [1/3], Step [3200/41412], Loss: 3.0360, Perplexity: 20.8209\n",
      "Epoch [1/3], Step [3300/41412], Loss: 3.2403, Perplexity: 25.5414\n",
      "Epoch [1/3], Step [3400/41412], Loss: 3.0748, Perplexity: 21.6450\n",
      "Epoch [1/3], Step [3500/41412], Loss: 2.5851, Perplexity: 13.2644\n",
      "Epoch [1/3], Step [3600/41412], Loss: 3.1287, Perplexity: 22.8450\n",
      "Epoch [1/3], Step [3700/41412], Loss: 3.0726, Perplexity: 21.5979\n",
      "Epoch [1/3], Step [3800/41412], Loss: 3.9138, Perplexity: 50.09110\n",
      "Epoch [1/3], Step [3900/41412], Loss: 2.7218, Perplexity: 15.2074\n",
      "Epoch [1/3], Step [4000/41412], Loss: 3.8015, Perplexity: 44.7671\n",
      "Epoch [1/3], Step [4100/41412], Loss: 3.1467, Perplexity: 23.2584\n",
      "Epoch [1/3], Step [4200/41412], Loss: 2.5702, Perplexity: 13.0689\n",
      "Epoch [1/3], Step [4300/41412], Loss: 2.8963, Perplexity: 18.1061\n",
      "Epoch [1/3], Step [4400/41412], Loss: 3.3645, Perplexity: 28.9185\n",
      "Epoch [1/3], Step [4500/41412], Loss: 2.5182, Perplexity: 12.4068\n",
      "Epoch [1/3], Step [4600/41412], Loss: 3.0245, Perplexity: 20.5834\n",
      "Epoch [1/3], Step [4700/41412], Loss: 3.1103, Perplexity: 22.4276\n",
      "Epoch [1/3], Step [4800/41412], Loss: 2.8291, Perplexity: 16.9299\n",
      "Epoch [1/3], Step [4900/41412], Loss: 2.9519, Perplexity: 19.1427\n",
      "Epoch [1/3], Step [5000/41412], Loss: 2.8866, Perplexity: 17.93295\n",
      "Epoch [1/3], Step [5100/41412], Loss: 2.7873, Perplexity: 16.2370\n",
      "Epoch [1/3], Step [5200/41412], Loss: 3.3044, Perplexity: 27.2323\n",
      "Epoch [1/3], Step [5300/41412], Loss: 3.2000, Perplexity: 24.5337\n",
      "Epoch [1/3], Step [5400/41412], Loss: 3.1108, Perplexity: 22.4387\n",
      "Epoch [1/3], Step [5500/41412], Loss: 3.1157, Perplexity: 22.5495\n",
      "Epoch [1/3], Step [5600/41412], Loss: 3.7414, Perplexity: 42.1570\n",
      "Epoch [1/3], Step [5700/41412], Loss: 2.6146, Perplexity: 13.6615\n",
      "Epoch [1/3], Step [5800/41412], Loss: 3.6388, Perplexity: 38.0453\n",
      "Epoch [1/3], Step [5900/41412], Loss: 2.6972, Perplexity: 14.8389\n",
      "Epoch [1/3], Step [6000/41412], Loss: 3.1957, Perplexity: 24.4284\n",
      "Epoch [1/3], Step [6100/41412], Loss: 3.8254, Perplexity: 45.8504\n",
      "Epoch [1/3], Step [6200/41412], Loss: 3.2704, Perplexity: 26.3209\n",
      "Epoch [1/3], Step [6300/41412], Loss: 3.3973, Perplexity: 29.8827\n",
      "Epoch [1/3], Step [6400/41412], Loss: 2.7536, Perplexity: 15.6991\n",
      "Epoch [1/3], Step [6500/41412], Loss: 2.7076, Perplexity: 14.9939\n",
      "Epoch [1/3], Step [6600/41412], Loss: 2.4096, Perplexity: 11.1300\n",
      "Epoch [1/3], Step [6700/41412], Loss: 3.0196, Perplexity: 20.4821\n",
      "Epoch [1/3], Step [6800/41412], Loss: 2.7536, Perplexity: 15.6994\n",
      "Epoch [1/3], Step [6900/41412], Loss: 3.1847, Perplexity: 24.1590\n",
      "Epoch [1/3], Step [7000/41412], Loss: 2.8393, Perplexity: 17.1029\n",
      "Epoch [1/3], Step [7100/41412], Loss: 2.7939, Perplexity: 16.3443\n",
      "Epoch [1/3], Step [7200/41412], Loss: 2.9915, Perplexity: 19.9157\n",
      "Epoch [1/3], Step [7300/41412], Loss: 3.6824, Perplexity: 39.7415\n",
      "Epoch [1/3], Step [7400/41412], Loss: 2.5718, Perplexity: 13.0897\n",
      "Epoch [1/3], Step [7500/41412], Loss: 2.5266, Perplexity: 12.5109\n",
      "Epoch [1/3], Step [7600/41412], Loss: 2.7844, Perplexity: 16.1897\n",
      "Epoch [1/3], Step [7700/41412], Loss: 2.3847, Perplexity: 10.8557\n",
      "Epoch [1/3], Step [7800/41412], Loss: 3.0163, Perplexity: 20.4148\n",
      "Epoch [1/3], Step [7900/41412], Loss: 3.0802, Perplexity: 21.7627\n",
      "Epoch [1/3], Step [8000/41412], Loss: 2.8166, Perplexity: 16.7191\n",
      "Epoch [1/3], Step [8100/41412], Loss: 3.2441, Perplexity: 25.6398\n",
      "Epoch [1/3], Step [8200/41412], Loss: 3.2912, Perplexity: 26.8760\n",
      "Epoch [1/3], Step [8300/41412], Loss: 2.9008, Perplexity: 18.1894\n",
      "Epoch [1/3], Step [8400/41412], Loss: 3.1578, Perplexity: 23.5196\n",
      "Epoch [1/3], Step [8500/41412], Loss: 2.1829, Perplexity: 8.87177\n",
      "Epoch [1/3], Step [8600/41412], Loss: 2.6770, Perplexity: 14.5408\n",
      "Epoch [1/3], Step [8700/41412], Loss: 2.9473, Perplexity: 19.05539\n",
      "Epoch [1/3], Step [8800/41412], Loss: 2.3306, Perplexity: 10.2836\n",
      "Epoch [1/3], Step [8900/41412], Loss: 3.0813, Perplexity: 21.7859\n",
      "Epoch [1/3], Step [9000/41412], Loss: 2.6325, Perplexity: 13.9079\n",
      "Epoch [1/3], Step [9100/41412], Loss: 2.3503, Perplexity: 10.4885\n",
      "Epoch [1/3], Step [9200/41412], Loss: 3.3017, Perplexity: 27.1586\n",
      "Epoch [1/3], Step [9300/41412], Loss: 2.1906, Perplexity: 8.94025\n",
      "Epoch [1/3], Step [9400/41412], Loss: 2.7933, Perplexity: 16.3352\n",
      "Epoch [1/3], Step [9500/41412], Loss: 2.8934, Perplexity: 18.0547\n",
      "Epoch [1/3], Step [9600/41412], Loss: 2.3003, Perplexity: 9.97764\n",
      "Epoch [1/3], Step [9700/41412], Loss: 2.3876, Perplexity: 10.88785\n",
      "Epoch [1/3], Step [9800/41412], Loss: 2.5391, Perplexity: 12.6677\n",
      "Epoch [1/3], Step [9900/41412], Loss: 3.0517, Perplexity: 21.1506\n",
      "Epoch [1/3], Step [10000/41412], Loss: 3.1612, Perplexity: 23.5984\n",
      "Epoch [1/3], Step [10100/41412], Loss: 2.6402, Perplexity: 14.0163\n",
      "Epoch [1/3], Step [10200/41412], Loss: 2.7427, Perplexity: 15.5289\n",
      "Epoch [1/3], Step [10300/41412], Loss: 2.7414, Perplexity: 15.5089\n",
      "Epoch [1/3], Step [10400/41412], Loss: 3.2530, Perplexity: 25.8684\n",
      "Epoch [1/3], Step [10500/41412], Loss: 2.7696, Perplexity: 15.9518\n",
      "Epoch [1/3], Step [10600/41412], Loss: 2.5846, Perplexity: 13.2575\n",
      "Epoch [1/3], Step [10700/41412], Loss: 2.9579, Perplexity: 19.2578\n",
      "Epoch [1/3], Step [10800/41412], Loss: 2.3719, Perplexity: 10.7173\n",
      "Epoch [1/3], Step [10900/41412], Loss: 2.3493, Perplexity: 10.4779\n",
      "Epoch [1/3], Step [11000/41412], Loss: 2.8663, Perplexity: 17.5724\n",
      "Epoch [1/3], Step [11100/41412], Loss: 2.5954, Perplexity: 13.4024\n",
      "Epoch [1/3], Step [11200/41412], Loss: 2.2816, Perplexity: 9.79273\n",
      "Epoch [1/3], Step [11300/41412], Loss: 2.7924, Perplexity: 16.3200\n",
      "Epoch [1/3], Step [11400/41412], Loss: 3.0163, Perplexity: 20.41650\n",
      "Epoch [1/3], Step [11500/41412], Loss: 2.9611, Perplexity: 19.3195\n",
      "Epoch [1/3], Step [11600/41412], Loss: 2.9478, Perplexity: 19.0634\n",
      "Epoch [1/3], Step [11700/41412], Loss: 2.4169, Perplexity: 11.21082\n",
      "Epoch [1/3], Step [11800/41412], Loss: 2.4317, Perplexity: 11.3781\n",
      "Epoch [1/3], Step [11900/41412], Loss: 2.4053, Perplexity: 11.0812\n",
      "Epoch [1/3], Step [12000/41412], Loss: 2.1915, Perplexity: 8.94908\n",
      "Epoch [1/3], Step [12100/41412], Loss: 2.6182, Perplexity: 13.7117\n",
      "Epoch [1/3], Step [12200/41412], Loss: 2.4204, Perplexity: 11.2500\n",
      "Epoch [1/3], Step [12300/41412], Loss: 3.6182, Perplexity: 37.2702\n",
      "Epoch [1/3], Step [12400/41412], Loss: 2.2767, Perplexity: 9.744975\n",
      "Epoch [1/3], Step [12500/41412], Loss: 2.0652, Perplexity: 7.88694\n",
      "Epoch [1/3], Step [12600/41412], Loss: 3.2259, Perplexity: 25.17759\n",
      "Epoch [1/3], Step [12700/41412], Loss: 3.2643, Perplexity: 26.1619\n",
      "Epoch [1/3], Step [12800/41412], Loss: 2.2909, Perplexity: 9.88429\n",
      "Epoch [1/3], Step [12900/41412], Loss: 3.6054, Perplexity: 36.7967\n",
      "Epoch [1/3], Step [13000/41412], Loss: 2.3165, Perplexity: 10.1405\n",
      "Epoch [1/3], Step [13100/41412], Loss: 3.0718, Perplexity: 21.5796\n",
      "Epoch [1/3], Step [13200/41412], Loss: 2.6432, Perplexity: 14.0583\n",
      "Epoch [1/3], Step [13300/41412], Loss: 3.6469, Perplexity: 38.3552\n",
      "Epoch [1/3], Step [13400/41412], Loss: 3.1947, Perplexity: 24.4033\n",
      "Epoch [1/3], Step [13500/41412], Loss: 2.7051, Perplexity: 14.9553\n",
      "Epoch [1/3], Step [13600/41412], Loss: 2.8348, Perplexity: 17.0275\n",
      "Epoch [1/3], Step [13700/41412], Loss: 1.8274, Perplexity: 6.21787\n",
      "Epoch [1/3], Step [13800/41412], Loss: 3.0089, Perplexity: 20.2643\n",
      "Epoch [1/3], Step [13900/41412], Loss: 2.7584, Perplexity: 15.7743\n",
      "Epoch [1/3], Step [14000/41412], Loss: 2.7410, Perplexity: 15.5020\n",
      "Epoch [1/3], Step [14100/41412], Loss: 2.3868, Perplexity: 10.8788\n",
      "Epoch [1/3], Step [14200/41412], Loss: 2.2700, Perplexity: 9.67970\n",
      "Epoch [1/3], Step [14300/41412], Loss: 2.7327, Perplexity: 15.3737\n",
      "Epoch [1/3], Step [14400/41412], Loss: 2.6258, Perplexity: 13.8163\n",
      "Epoch [1/3], Step [14500/41412], Loss: 2.5168, Perplexity: 12.3883\n",
      "Epoch [1/3], Step [14600/41412], Loss: 2.8708, Perplexity: 17.6504\n",
      "Epoch [1/3], Step [14700/41412], Loss: 2.1502, Perplexity: 8.58649\n",
      "Epoch [1/3], Step [14800/41412], Loss: 2.8985, Perplexity: 18.1464\n",
      "Epoch [1/3], Step [14900/41412], Loss: 2.1814, Perplexity: 8.85870\n",
      "Epoch [1/3], Step [15000/41412], Loss: 2.3758, Perplexity: 10.7592\n",
      "Epoch [1/3], Step [15100/41412], Loss: 2.4024, Perplexity: 11.0497\n",
      "Epoch [1/3], Step [15200/41412], Loss: 2.1315, Perplexity: 8.42795\n",
      "Epoch [1/3], Step [15300/41412], Loss: 3.2052, Perplexity: 24.6595\n",
      "Epoch [1/3], Step [15400/41412], Loss: 2.6740, Perplexity: 14.4976\n",
      "Epoch [1/3], Step [15500/41412], Loss: 2.2552, Perplexity: 9.53764\n",
      "Epoch [1/3], Step [15600/41412], Loss: 2.3083, Perplexity: 10.0571\n",
      "Epoch [1/3], Step [15700/41412], Loss: 3.5317, Perplexity: 34.1814\n",
      "Epoch [1/3], Step [15800/41412], Loss: 2.1537, Perplexity: 8.61675\n",
      "Epoch [1/3], Step [15900/41412], Loss: 2.3709, Perplexity: 10.7068\n",
      "Epoch [1/3], Step [16000/41412], Loss: 2.4142, Perplexity: 11.1806\n",
      "Epoch [1/3], Step [16100/41412], Loss: 3.6092, Perplexity: 36.9373\n",
      "Epoch [1/3], Step [16200/41412], Loss: 2.2875, Perplexity: 9.84989\n",
      "Epoch [1/3], Step [16300/41412], Loss: 2.2065, Perplexity: 9.08399\n",
      "Epoch [1/3], Step [16400/41412], Loss: 2.5423, Perplexity: 12.7094\n",
      "Epoch [1/3], Step [16500/41412], Loss: 2.2335, Perplexity: 9.33255\n",
      "Epoch [1/3], Step [16600/41412], Loss: 2.4765, Perplexity: 11.8996\n",
      "Epoch [1/3], Step [16700/41412], Loss: 2.3699, Perplexity: 10.6966\n",
      "Epoch [1/3], Step [16800/41412], Loss: 2.4709, Perplexity: 11.8328\n",
      "Epoch [1/3], Step [16900/41412], Loss: 2.1691, Perplexity: 8.75011\n",
      "Epoch [1/3], Step [17000/41412], Loss: 1.9780, Perplexity: 7.22866\n",
      "Epoch [1/3], Step [17100/41412], Loss: 2.4852, Perplexity: 12.0036\n",
      "Epoch [1/3], Step [17200/41412], Loss: 2.6265, Perplexity: 13.8248\n",
      "Epoch [1/3], Step [17300/41412], Loss: 2.4591, Perplexity: 11.6946\n",
      "Epoch [1/3], Step [17400/41412], Loss: 3.5679, Perplexity: 35.4436\n",
      "Epoch [1/3], Step [17500/41412], Loss: 2.7567, Perplexity: 15.7482\n",
      "Epoch [1/3], Step [17600/41412], Loss: 2.7167, Perplexity: 15.1298\n",
      "Epoch [1/3], Step [17700/41412], Loss: 2.8305, Perplexity: 16.9539\n",
      "Epoch [1/3], Step [17800/41412], Loss: 2.4230, Perplexity: 11.2802\n",
      "Epoch [1/3], Step [17900/41412], Loss: 2.3977, Perplexity: 10.9981\n",
      "Epoch [1/3], Step [18000/41412], Loss: 2.7177, Perplexity: 15.1448\n",
      "Epoch [1/3], Step [18100/41412], Loss: 2.0810, Perplexity: 8.01250\n",
      "Epoch [1/3], Step [18200/41412], Loss: 2.1981, Perplexity: 9.00792\n",
      "Epoch [1/3], Step [18300/41412], Loss: 2.5351, Perplexity: 12.6175\n",
      "Epoch [1/3], Step [18400/41412], Loss: 3.0154, Perplexity: 20.3981\n",
      "Epoch [1/3], Step [18500/41412], Loss: 2.6361, Perplexity: 13.9583\n",
      "Epoch [1/3], Step [18600/41412], Loss: 2.5271, Perplexity: 12.5172\n",
      "Epoch [1/3], Step [18700/41412], Loss: 2.5359, Perplexity: 12.6277\n",
      "Epoch [1/3], Step [18800/41412], Loss: 2.8817, Perplexity: 17.8447\n",
      "Epoch [1/3], Step [18900/41412], Loss: 2.2279, Perplexity: 9.280718\n",
      "Epoch [1/3], Step [19000/41412], Loss: 2.7231, Perplexity: 15.2281\n",
      "Epoch [1/3], Step [19100/41412], Loss: 2.5471, Perplexity: 12.7706\n",
      "Epoch [1/3], Step [19200/41412], Loss: 2.3604, Perplexity: 10.5951\n",
      "Epoch [1/3], Step [19300/41412], Loss: 2.3959, Perplexity: 10.9781\n",
      "Epoch [1/3], Step [19400/41412], Loss: 1.9942, Perplexity: 7.34626\n",
      "Epoch [1/3], Step [19500/41412], Loss: 3.1868, Perplexity: 24.2113\n",
      "Epoch [1/3], Step [19600/41412], Loss: 2.3687, Perplexity: 10.6831\n",
      "Epoch [1/3], Step [19700/41412], Loss: 1.9658, Perplexity: 7.14043\n",
      "Epoch [1/3], Step [19800/41412], Loss: 2.8491, Perplexity: 17.2727\n",
      "Epoch [1/3], Step [19900/41412], Loss: 2.3691, Perplexity: 10.6877\n",
      "Epoch [1/3], Step [20000/41412], Loss: 2.4065, Perplexity: 11.0949\n",
      "Epoch [1/3], Step [20100/41412], Loss: 2.9507, Perplexity: 19.1189\n",
      "Epoch [1/3], Step [20200/41412], Loss: 2.7374, Perplexity: 15.4465\n",
      "Epoch [1/3], Step [20300/41412], Loss: 2.5202, Perplexity: 12.4313\n",
      "Epoch [1/3], Step [20400/41412], Loss: 3.8122, Perplexity: 45.2503\n",
      "Epoch [1/3], Step [20500/41412], Loss: 2.1598, Perplexity: 8.66917\n",
      "Epoch [1/3], Step [20600/41412], Loss: 2.0992, Perplexity: 8.15999\n",
      "Epoch [1/3], Step [20700/41412], Loss: 2.5279, Perplexity: 12.5273\n",
      "Epoch [1/3], Step [20800/41412], Loss: 3.3540, Perplexity: 28.6177\n",
      "Epoch [1/3], Step [20900/41412], Loss: 2.1690, Perplexity: 8.74973\n",
      "Epoch [1/3], Step [21000/41412], Loss: 3.0893, Perplexity: 21.9619\n",
      "Epoch [1/3], Step [21100/41412], Loss: 3.0061, Perplexity: 20.2082\n",
      "Epoch [1/3], Step [21200/41412], Loss: 2.7064, Perplexity: 14.9760\n",
      "Epoch [1/3], Step [21300/41412], Loss: 2.4094, Perplexity: 11.1276\n",
      "Epoch [1/3], Step [21400/41412], Loss: 2.8396, Perplexity: 17.10873\n",
      "Epoch [1/3], Step [21500/41412], Loss: 2.1505, Perplexity: 8.58889\n",
      "Epoch [1/3], Step [21600/41412], Loss: 2.3436, Perplexity: 10.4190\n",
      "Epoch [1/3], Step [21700/41412], Loss: 1.8602, Perplexity: 6.42523\n",
      "Epoch [1/3], Step [21800/41412], Loss: 2.2200, Perplexity: 9.20715\n",
      "Epoch [1/3], Step [21900/41412], Loss: 2.9611, Perplexity: 19.3191\n",
      "Epoch [1/3], Step [22000/41412], Loss: 3.5604, Perplexity: 35.1762\n",
      "Epoch [1/3], Step [22100/41412], Loss: 2.7153, Perplexity: 15.1086\n",
      "Epoch [1/3], Step [22200/41412], Loss: 2.8196, Perplexity: 16.7703\n",
      "Epoch [1/3], Step [22300/41412], Loss: 3.5106, Perplexity: 33.4671\n",
      "Epoch [1/3], Step [22400/41412], Loss: 2.6253, Perplexity: 13.8094\n",
      "Epoch [1/3], Step [22500/41412], Loss: 2.0633, Perplexity: 7.87216\n",
      "Epoch [1/3], Step [22600/41412], Loss: 2.5525, Perplexity: 12.8390\n",
      "Epoch [1/3], Step [22700/41412], Loss: 2.3100, Perplexity: 10.0747\n",
      "Epoch [1/3], Step [22800/41412], Loss: 2.3487, Perplexity: 10.4721\n",
      "Epoch [1/3], Step [22900/41412], Loss: 2.6532, Perplexity: 14.1987\n",
      "Epoch [1/3], Step [23000/41412], Loss: 2.0382, Perplexity: 7.67682\n",
      "Epoch [1/3], Step [23100/41412], Loss: 2.2751, Perplexity: 9.72862\n",
      "Epoch [1/3], Step [23200/41412], Loss: 2.5943, Perplexity: 13.3868\n",
      "Epoch [1/3], Step [23300/41412], Loss: 2.0601, Perplexity: 7.84653\n",
      "Epoch [1/3], Step [23400/41412], Loss: 2.0963, Perplexity: 8.13585\n",
      "Epoch [1/3], Step [23500/41412], Loss: 2.4231, Perplexity: 11.2803\n",
      "Epoch [1/3], Step [23600/41412], Loss: 2.2943, Perplexity: 9.91783\n",
      "Epoch [1/3], Step [23700/41412], Loss: 2.6396, Perplexity: 14.0075\n",
      "Epoch [1/3], Step [23800/41412], Loss: 2.9330, Perplexity: 18.7842\n",
      "Epoch [1/3], Step [23900/41412], Loss: 2.2057, Perplexity: 9.07635\n",
      "Epoch [1/3], Step [24000/41412], Loss: 3.0731, Perplexity: 21.6098\n",
      "Epoch [1/3], Step [24100/41412], Loss: 2.3548, Perplexity: 10.5359\n",
      "Epoch [1/3], Step [24200/41412], Loss: 2.1743, Perplexity: 8.79578\n",
      "Epoch [1/3], Step [24300/41412], Loss: 2.1315, Perplexity: 8.42725\n",
      "Epoch [1/3], Step [24400/41412], Loss: 2.4154, Perplexity: 11.1939\n",
      "Epoch [1/3], Step [24500/41412], Loss: 2.4666, Perplexity: 11.7825\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3], Step [24600/41412], Loss: 2.7594, Perplexity: 15.7899\n",
      "Epoch [1/3], Step [24700/41412], Loss: 2.2886, Perplexity: 9.86168\n",
      "Epoch [1/3], Step [24800/41412], Loss: 2.1905, Perplexity: 8.93968\n",
      "Epoch [1/3], Step [24900/41412], Loss: 2.5690, Perplexity: 13.0531\n",
      "Epoch [1/3], Step [25000/41412], Loss: 2.1076, Perplexity: 8.22844\n",
      "Epoch [1/3], Step [25100/41412], Loss: 2.3499, Perplexity: 10.4841\n",
      "Epoch [1/3], Step [25200/41412], Loss: 2.3476, Perplexity: 10.4606\n",
      "Epoch [1/3], Step [25300/41412], Loss: 2.2224, Perplexity: 9.22926\n",
      "Epoch [1/3], Step [25400/41412], Loss: 2.2301, Perplexity: 9.30086\n",
      "Epoch [1/3], Step [25500/41412], Loss: 1.9744, Perplexity: 7.20230\n",
      "Epoch [1/3], Step [25600/41412], Loss: 2.9351, Perplexity: 18.8225\n",
      "Epoch [1/3], Step [25700/41412], Loss: 2.3118, Perplexity: 10.0930\n",
      "Epoch [1/3], Step [25800/41412], Loss: 2.0397, Perplexity: 7.68838\n",
      "Epoch [1/3], Step [25900/41412], Loss: 2.2658, Perplexity: 9.63888\n",
      "Epoch [1/3], Step [26000/41412], Loss: 2.0248, Perplexity: 7.57498\n",
      "Epoch [1/3], Step [26100/41412], Loss: 2.4484, Perplexity: 11.5700\n",
      "Epoch [1/3], Step [26200/41412], Loss: 2.5129, Perplexity: 12.3405\n",
      "Epoch [1/3], Step [26300/41412], Loss: 2.2463, Perplexity: 9.45249\n",
      "Epoch [1/3], Step [26400/41412], Loss: 2.4765, Perplexity: 11.9000\n",
      "Epoch [1/3], Step [26500/41412], Loss: 2.9166, Perplexity: 18.47891\n",
      "Epoch [1/3], Step [26600/41412], Loss: 2.3916, Perplexity: 10.9309\n",
      "Epoch [1/3], Step [26700/41412], Loss: 2.4931, Perplexity: 12.0985\n",
      "Epoch [1/3], Step [26800/41412], Loss: 2.6207, Perplexity: 13.7451\n",
      "Epoch [1/3], Step [26900/41412], Loss: 2.3076, Perplexity: 10.0502\n",
      "Epoch [1/3], Step [27000/41412], Loss: 2.7029, Perplexity: 14.9230\n",
      "Epoch [1/3], Step [27100/41412], Loss: 2.6828, Perplexity: 14.6255\n",
      "Epoch [1/3], Step [27200/41412], Loss: 2.3813, Perplexity: 10.8195\n",
      "Epoch [1/3], Step [27300/41412], Loss: 2.8221, Perplexity: 16.8126\n",
      "Epoch [1/3], Step [27400/41412], Loss: 2.0648, Perplexity: 7.88356\n",
      "Epoch [1/3], Step [27500/41412], Loss: 2.6137, Perplexity: 13.6500\n",
      "Epoch [1/3], Step [27600/41412], Loss: 2.8070, Perplexity: 16.5609\n",
      "Epoch [1/3], Step [27700/41412], Loss: 2.2736, Perplexity: 9.71434\n",
      "Epoch [1/3], Step [27800/41412], Loss: 2.2173, Perplexity: 9.18295\n",
      "Epoch [1/3], Step [27900/41412], Loss: 2.3750, Perplexity: 10.7514\n",
      "Epoch [1/3], Step [28000/41412], Loss: 2.6122, Perplexity: 13.6284\n",
      "Epoch [1/3], Step [28100/41412], Loss: 2.4425, Perplexity: 11.5020\n",
      "Epoch [1/3], Step [28200/41412], Loss: 2.1856, Perplexity: 8.89628\n",
      "Epoch [1/3], Step [28300/41412], Loss: 2.2962, Perplexity: 9.93641\n",
      "Epoch [1/3], Step [28400/41412], Loss: 2.5073, Perplexity: 12.2722\n",
      "Epoch [1/3], Step [28500/41412], Loss: 2.5319, Perplexity: 12.5774\n",
      "Epoch [1/3], Step [28600/41412], Loss: 2.5076, Perplexity: 12.2758\n",
      "Epoch [1/3], Step [28700/41412], Loss: 2.3998, Perplexity: 11.0211\n",
      "Epoch [1/3], Step [28800/41412], Loss: 2.2568, Perplexity: 9.55227\n",
      "Epoch [1/3], Step [28900/41412], Loss: 2.6106, Perplexity: 13.6077\n",
      "Epoch [1/3], Step [29000/41412], Loss: 3.2920, Perplexity: 26.8970\n",
      "Epoch [1/3], Step [29100/41412], Loss: 2.5429, Perplexity: 12.7165\n",
      "Epoch [1/3], Step [29200/41412], Loss: 2.3949, Perplexity: 10.9671\n",
      "Epoch [1/3], Step [29300/41412], Loss: 3.1371, Perplexity: 23.0361\n",
      "Epoch [1/3], Step [29400/41412], Loss: 1.9877, Perplexity: 7.29854\n",
      "Epoch [1/3], Step [29500/41412], Loss: 2.6254, Perplexity: 13.8100\n",
      "Epoch [1/3], Step [29600/41412], Loss: 3.1359, Perplexity: 23.0089\n",
      "Epoch [1/3], Step [29700/41412], Loss: 2.5241, Perplexity: 12.4794\n",
      "Epoch [1/3], Step [29800/41412], Loss: 2.7677, Perplexity: 15.9218\n",
      "Epoch [1/3], Step [29900/41412], Loss: 2.4824, Perplexity: 11.9698\n",
      "Epoch [1/3], Step [30000/41412], Loss: 2.3568, Perplexity: 10.5574\n",
      "Epoch [1/3], Step [30100/41412], Loss: 2.3409, Perplexity: 10.3905\n",
      "Epoch [1/3], Step [30200/41412], Loss: 2.0682, Perplexity: 7.91020\n",
      "Epoch [1/3], Step [30300/41412], Loss: 2.4214, Perplexity: 11.2617\n",
      "Epoch [1/3], Step [30400/41412], Loss: 2.3636, Perplexity: 10.6288\n",
      "Epoch [1/3], Step [30500/41412], Loss: 2.5443, Perplexity: 12.7340\n",
      "Epoch [1/3], Step [30600/41412], Loss: 2.8645, Perplexity: 17.5406\n",
      "Epoch [1/3], Step [30700/41412], Loss: 2.6557, Perplexity: 14.2343\n",
      "Epoch [1/3], Step [30800/41412], Loss: 2.0441, Perplexity: 7.72205\n",
      "Epoch [1/3], Step [30900/41412], Loss: 2.3767, Perplexity: 10.7693\n",
      "Epoch [1/3], Step [31000/41412], Loss: 1.5590, Perplexity: 4.75420\n",
      "Epoch [1/3], Step [31100/41412], Loss: 1.7225, Perplexity: 5.59850\n",
      "Epoch [1/3], Step [31200/41412], Loss: 2.2364, Perplexity: 9.35966\n",
      "Epoch [1/3], Step [31300/41412], Loss: 2.9496, Perplexity: 19.0981\n",
      "Epoch [1/3], Step [31400/41412], Loss: 2.1903, Perplexity: 8.93768\n",
      "Epoch [1/3], Step [31500/41412], Loss: 2.0495, Perplexity: 7.76378\n",
      "Epoch [1/3], Step [31600/41412], Loss: 1.9225, Perplexity: 6.83789\n",
      "Epoch [1/3], Step [31700/41412], Loss: 3.2666, Perplexity: 26.2219\n",
      "Epoch [1/3], Step [31800/41412], Loss: 2.4172, Perplexity: 11.2147\n",
      "Epoch [1/3], Step [31900/41412], Loss: 2.2448, Perplexity: 9.43864\n",
      "Epoch [1/3], Step [32000/41412], Loss: 2.1431, Perplexity: 8.52610\n",
      "Epoch [1/3], Step [32100/41412], Loss: 1.8678, Perplexity: 6.47430\n",
      "Epoch [1/3], Step [32200/41412], Loss: 1.8433, Perplexity: 6.31753\n",
      "Epoch [1/3], Step [32300/41412], Loss: 2.5652, Perplexity: 13.0035\n",
      "Epoch [1/3], Step [32400/41412], Loss: 2.2676, Perplexity: 9.65650\n",
      "Epoch [1/3], Step [32500/41412], Loss: 2.4786, Perplexity: 11.9245\n",
      "Epoch [1/3], Step [32600/41412], Loss: 2.0294, Perplexity: 7.60955\n",
      "Epoch [1/3], Step [32700/41412], Loss: 2.1299, Perplexity: 8.41436\n",
      "Epoch [1/3], Step [32800/41412], Loss: 2.6497, Perplexity: 14.1493\n",
      "Epoch [1/3], Step [32900/41412], Loss: 2.0957, Perplexity: 8.13136\n",
      "Epoch [1/3], Step [33000/41412], Loss: 1.8292, Perplexity: 6.22902\n",
      "Epoch [1/3], Step [33100/41412], Loss: 2.1892, Perplexity: 8.92768\n",
      "Epoch [1/3], Step [33200/41412], Loss: 2.5124, Perplexity: 12.3347\n",
      "Epoch [1/3], Step [33300/41412], Loss: 2.6357, Perplexity: 13.9534\n",
      "Epoch [1/3], Step [33400/41412], Loss: 1.8787, Perplexity: 6.54517\n",
      "Epoch [1/3], Step [33500/41412], Loss: 2.5517, Perplexity: 12.8288\n",
      "Epoch [1/3], Step [33600/41412], Loss: 2.6791, Perplexity: 14.5722\n",
      "Epoch [1/3], Step [33700/41412], Loss: 2.3126, Perplexity: 10.1004\n",
      "Epoch [1/3], Step [33800/41412], Loss: 2.0291, Perplexity: 7.60731\n",
      "Epoch [1/3], Step [33900/41412], Loss: 2.7395, Perplexity: 15.4789\n",
      "Epoch [1/3], Step [34000/41412], Loss: 2.1301, Perplexity: 8.41555\n",
      "Epoch [1/3], Step [34100/41412], Loss: 2.5874, Perplexity: 13.2958\n",
      "Epoch [1/3], Step [34200/41412], Loss: 2.8977, Perplexity: 18.1328\n",
      "Epoch [1/3], Step [34300/41412], Loss: 1.9861, Perplexity: 7.28733\n",
      "Epoch [1/3], Step [34400/41412], Loss: 2.0003, Perplexity: 7.39103\n",
      "Epoch [1/3], Step [34500/41412], Loss: 2.7775, Perplexity: 16.0791\n",
      "Epoch [1/3], Step [34600/41412], Loss: 2.4359, Perplexity: 11.4260\n",
      "Epoch [1/3], Step [34700/41412], Loss: 2.2134, Perplexity: 9.14706\n",
      "Epoch [1/3], Step [34800/41412], Loss: 2.5923, Perplexity: 13.3610\n",
      "Epoch [1/3], Step [34900/41412], Loss: 2.3879, Perplexity: 10.8904\n",
      "Epoch [1/3], Step [35000/41412], Loss: 2.2376, Perplexity: 9.37050\n",
      "Epoch [1/3], Step [35100/41412], Loss: 2.7261, Perplexity: 15.2728\n",
      "Epoch [1/3], Step [35200/41412], Loss: 2.0886, Perplexity: 8.07364\n",
      "Epoch [1/3], Step [35300/41412], Loss: 3.2656, Perplexity: 26.1968\n",
      "Epoch [1/3], Step [35400/41412], Loss: 2.2400, Perplexity: 9.39305\n",
      "Epoch [1/3], Step [35500/41412], Loss: 2.1809, Perplexity: 8.85450\n",
      "Epoch [1/3], Step [35600/41412], Loss: 2.9544, Perplexity: 19.1893\n",
      "Epoch [1/3], Step [35700/41412], Loss: 2.1836, Perplexity: 8.87805\n",
      "Epoch [1/3], Step [35800/41412], Loss: 2.4903, Perplexity: 12.0649\n",
      "Epoch [1/3], Step [35900/41412], Loss: 2.6001, Perplexity: 13.4652\n",
      "Epoch [1/3], Step [36000/41412], Loss: 2.2664, Perplexity: 9.64432\n",
      "Epoch [1/3], Step [36100/41412], Loss: 2.1250, Perplexity: 8.37250\n",
      "Epoch [1/3], Step [36200/41412], Loss: 2.6411, Perplexity: 14.0292\n",
      "Epoch [1/3], Step [36300/41412], Loss: 2.5319, Perplexity: 12.5769\n",
      "Epoch [1/3], Step [36400/41412], Loss: 2.5322, Perplexity: 12.5810\n",
      "Epoch [1/3], Step [36500/41412], Loss: 2.7097, Perplexity: 15.0246\n",
      "Epoch [1/3], Step [36600/41412], Loss: 2.2157, Perplexity: 9.16819\n",
      "Epoch [1/3], Step [36700/41412], Loss: 2.3684, Perplexity: 10.6801\n",
      "Epoch [1/3], Step [36800/41412], Loss: 2.4912, Perplexity: 12.07573\n",
      "Epoch [1/3], Step [36900/41412], Loss: 2.6485, Perplexity: 14.1332\n",
      "Epoch [1/3], Step [37000/41412], Loss: 2.0651, Perplexity: 7.88582\n",
      "Epoch [1/3], Step [37100/41412], Loss: 2.0671, Perplexity: 7.90203\n",
      "Epoch [1/3], Step [37200/41412], Loss: 1.5771, Perplexity: 4.84112\n",
      "Epoch [1/3], Step [37300/41412], Loss: 2.3875, Perplexity: 10.8865\n",
      "Epoch [1/3], Step [37400/41412], Loss: 2.5504, Perplexity: 12.8122\n",
      "Epoch [1/3], Step [37500/41412], Loss: 3.2793, Perplexity: 26.5579\n",
      "Epoch [1/3], Step [37600/41412], Loss: 3.1337, Perplexity: 22.9591\n",
      "Epoch [1/3], Step [37700/41412], Loss: 2.5097, Perplexity: 12.3012\n",
      "Epoch [1/3], Step [37800/41412], Loss: 2.7274, Perplexity: 15.2931\n",
      "Epoch [1/3], Step [37900/41412], Loss: 2.2816, Perplexity: 9.79227\n",
      "Epoch [1/3], Step [38000/41412], Loss: 1.8315, Perplexity: 6.24340\n",
      "Epoch [1/3], Step [38100/41412], Loss: 2.4330, Perplexity: 11.3930\n",
      "Epoch [1/3], Step [38200/41412], Loss: 2.1486, Perplexity: 8.57296\n",
      "Epoch [1/3], Step [38300/41412], Loss: 2.2681, Perplexity: 9.66147\n",
      "Epoch [1/3], Step [38400/41412], Loss: 2.5257, Perplexity: 12.4992\n",
      "Epoch [1/3], Step [38500/41412], Loss: 2.0387, Perplexity: 7.68031\n",
      "Epoch [1/3], Step [38600/41412], Loss: 2.4238, Perplexity: 11.2883\n",
      "Epoch [1/3], Step [38700/41412], Loss: 2.6863, Perplexity: 14.6778\n",
      "Epoch [1/3], Step [38800/41412], Loss: 1.9959, Perplexity: 7.35902\n",
      "Epoch [1/3], Step [38900/41412], Loss: 2.3069, Perplexity: 10.0437\n",
      "Epoch [1/3], Step [39000/41412], Loss: 2.3784, Perplexity: 10.7879\n",
      "Epoch [1/3], Step [39100/41412], Loss: 1.7530, Perplexity: 5.77216\n",
      "Epoch [1/3], Step [39200/41412], Loss: 2.8504, Perplexity: 17.2949\n",
      "Epoch [1/3], Step [39300/41412], Loss: 2.3162, Perplexity: 10.1374\n",
      "Epoch [1/3], Step [39400/41412], Loss: 2.0553, Perplexity: 7.80923\n",
      "Epoch [1/3], Step [39500/41412], Loss: 2.2562, Perplexity: 9.54648\n",
      "Epoch [1/3], Step [39600/41412], Loss: 1.9051, Perplexity: 6.71995\n",
      "Epoch [1/3], Step [39700/41412], Loss: 2.5452, Perplexity: 12.7452\n",
      "Epoch [1/3], Step [39800/41412], Loss: 2.0991, Perplexity: 8.15845\n",
      "Epoch [1/3], Step [39900/41412], Loss: 2.1792, Perplexity: 8.83884\n",
      "Epoch [1/3], Step [40000/41412], Loss: 1.8913, Perplexity: 6.62817\n",
      "Epoch [1/3], Step [40100/41412], Loss: 2.5421, Perplexity: 12.7069\n",
      "Epoch [1/3], Step [40200/41412], Loss: 2.4180, Perplexity: 11.2235\n",
      "Epoch [1/3], Step [40300/41412], Loss: 2.5968, Perplexity: 13.4202\n",
      "Epoch [1/3], Step [40400/41412], Loss: 2.0975, Perplexity: 8.14609\n",
      "Epoch [1/3], Step [40500/41412], Loss: 2.5567, Perplexity: 12.8938\n",
      "Epoch [1/3], Step [40600/41412], Loss: 3.0926, Perplexity: 22.0333\n",
      "Epoch [1/3], Step [40700/41412], Loss: 2.4211, Perplexity: 11.2584\n",
      "Epoch [1/3], Step [40800/41412], Loss: 2.7351, Perplexity: 15.4117\n",
      "Epoch [1/3], Step [40900/41412], Loss: 2.3497, Perplexity: 10.4820\n",
      "Epoch [1/3], Step [41000/41412], Loss: 1.9476, Perplexity: 7.01171\n",
      "Epoch [1/3], Step [41100/41412], Loss: 2.1222, Perplexity: 8.34971\n",
      "Epoch [1/3], Step [41200/41412], Loss: 2.5408, Perplexity: 12.6896\n",
      "Epoch [1/3], Step [41300/41412], Loss: 2.0596, Perplexity: 7.84257\n",
      "Epoch [1/3], Step [41400/41412], Loss: 2.2972, Perplexity: 9.94661\n",
      "Epoch [2/3], Step [100/41412], Loss: 1.8506, Perplexity: 6.3637784\n",
      "Epoch [2/3], Step [200/41412], Loss: 2.0997, Perplexity: 8.16362\n",
      "Epoch [2/3], Step [300/41412], Loss: 2.5836, Perplexity: 13.2449\n",
      "Epoch [2/3], Step [400/41412], Loss: 1.6830, Perplexity: 5.38149\n",
      "Epoch [2/3], Step [500/41412], Loss: 2.6200, Perplexity: 13.7357\n",
      "Epoch [2/3], Step [600/41412], Loss: 2.4423, Perplexity: 11.4997\n",
      "Epoch [2/3], Step [700/41412], Loss: 2.7986, Perplexity: 16.4214\n",
      "Epoch [2/3], Step [800/41412], Loss: 2.8758, Perplexity: 17.7401\n",
      "Epoch [2/3], Step [900/41412], Loss: 2.7739, Perplexity: 16.0208\n",
      "Epoch [2/3], Step [1000/41412], Loss: 2.2459, Perplexity: 9.4489\n",
      "Epoch [2/3], Step [1100/41412], Loss: 2.4940, Perplexity: 12.1102\n",
      "Epoch [2/3], Step [1200/41412], Loss: 2.3530, Perplexity: 10.5168\n",
      "Epoch [2/3], Step [1300/41412], Loss: 2.2820, Perplexity: 9.79670\n",
      "Epoch [2/3], Step [1400/41412], Loss: 2.1302, Perplexity: 8.41652\n",
      "Epoch [2/3], Step [1500/41412], Loss: 2.4387, Perplexity: 11.4583\n",
      "Epoch [2/3], Step [1600/41412], Loss: 2.5747, Perplexity: 13.1276\n",
      "Epoch [2/3], Step [1700/41412], Loss: 2.3956, Perplexity: 10.9753\n",
      "Epoch [2/3], Step [1800/41412], Loss: 2.5015, Perplexity: 12.2004\n",
      "Epoch [2/3], Step [1900/41412], Loss: 2.4816, Perplexity: 11.9605\n",
      "Epoch [2/3], Step [2000/41412], Loss: 2.7284, Perplexity: 15.3083\n",
      "Epoch [2/3], Step [2100/41412], Loss: 1.9058, Perplexity: 6.72467\n",
      "Epoch [2/3], Step [2200/41412], Loss: 2.2099, Perplexity: 9.11442\n",
      "Epoch [2/3], Step [2300/41412], Loss: 1.8868, Perplexity: 6.59826\n",
      "Epoch [2/3], Step [2400/41412], Loss: 2.3810, Perplexity: 10.8152\n",
      "Epoch [2/3], Step [2500/41412], Loss: 2.2602, Perplexity: 9.58543\n",
      "Epoch [2/3], Step [2600/41412], Loss: 2.0121, Perplexity: 7.47939\n",
      "Epoch [2/3], Step [2700/41412], Loss: 1.9819, Perplexity: 7.25683\n",
      "Epoch [2/3], Step [2800/41412], Loss: 2.2642, Perplexity: 9.62335\n",
      "Epoch [2/3], Step [2900/41412], Loss: 2.3663, Perplexity: 10.6582\n",
      "Epoch [2/3], Step [3000/41412], Loss: 1.9113, Perplexity: 6.76205\n",
      "Epoch [2/3], Step [3100/41412], Loss: 2.3429, Perplexity: 10.4110\n",
      "Epoch [2/3], Step [3200/41412], Loss: 2.1448, Perplexity: 8.54011\n",
      "Epoch [2/3], Step [3300/41412], Loss: 2.2158, Perplexity: 9.16866\n",
      "Epoch [2/3], Step [3400/41412], Loss: 2.8937, Perplexity: 18.0593\n",
      "Epoch [2/3], Step [3500/41412], Loss: 3.0790, Perplexity: 21.7364\n",
      "Epoch [2/3], Step [3600/41412], Loss: 2.5687, Perplexity: 13.0485\n",
      "Epoch [2/3], Step [3700/41412], Loss: 2.2888, Perplexity: 9.86319\n",
      "Epoch [2/3], Step [3800/41412], Loss: 2.2272, Perplexity: 9.27350\n",
      "Epoch [2/3], Step [3900/41412], Loss: 2.6329, Perplexity: 13.9134\n",
      "Epoch [2/3], Step [4000/41412], Loss: 2.5102, Perplexity: 12.3069\n",
      "Epoch [2/3], Step [4100/41412], Loss: 2.3374, Perplexity: 10.3548\n",
      "Epoch [2/3], Step [4200/41412], Loss: 2.2456, Perplexity: 9.44635\n",
      "Epoch [2/3], Step [4300/41412], Loss: 2.1641, Perplexity: 8.70727\n",
      "Epoch [2/3], Step [4400/41412], Loss: 2.3320, Perplexity: 10.2990\n",
      "Epoch [2/3], Step [4500/41412], Loss: 1.8544, Perplexity: 6.38800\n",
      "Epoch [2/3], Step [4600/41412], Loss: 2.0907, Perplexity: 8.09107\n",
      "Epoch [2/3], Step [4700/41412], Loss: 2.3856, Perplexity: 10.8656\n",
      "Epoch [2/3], Step [4800/41412], Loss: 2.5078, Perplexity: 12.2775\n",
      "Epoch [2/3], Step [4900/41412], Loss: 3.0177, Perplexity: 20.4432\n",
      "Epoch [2/3], Step [5000/41412], Loss: 2.6668, Perplexity: 14.3940\n",
      "Epoch [2/3], Step [5100/41412], Loss: 1.7261, Perplexity: 5.61872\n",
      "Epoch [2/3], Step [5200/41412], Loss: 2.3920, Perplexity: 10.9354\n",
      "Epoch [2/3], Step [5300/41412], Loss: 2.3692, Perplexity: 10.6884\n",
      "Epoch [2/3], Step [5400/41412], Loss: 2.3588, Perplexity: 10.5782\n",
      "Epoch [2/3], Step [5500/41412], Loss: 2.5415, Perplexity: 12.6993\n",
      "Epoch [2/3], Step [5600/41412], Loss: 2.0868, Perplexity: 8.05900\n",
      "Epoch [2/3], Step [5700/41412], Loss: 2.2373, Perplexity: 9.36807\n",
      "Epoch [2/3], Step [5800/41412], Loss: 2.2509, Perplexity: 9.49672\n",
      "Epoch [2/3], Step [5900/41412], Loss: 2.4163, Perplexity: 11.2048\n",
      "Epoch [2/3], Step [6000/41412], Loss: 3.6226, Perplexity: 37.4333\n",
      "Epoch [2/3], Step [6100/41412], Loss: 1.9064, Perplexity: 6.72880\n",
      "Epoch [2/3], Step [6200/41412], Loss: 2.4506, Perplexity: 11.5953\n",
      "Epoch [2/3], Step [6300/41412], Loss: 2.6291, Perplexity: 13.8613\n",
      "Epoch [2/3], Step [6400/41412], Loss: 2.5390, Perplexity: 12.6672\n",
      "Epoch [2/3], Step [6500/41412], Loss: 2.5136, Perplexity: 12.3499\n",
      "Epoch [2/3], Step [6600/41412], Loss: 2.2863, Perplexity: 9.83892\n",
      "Epoch [2/3], Step [6700/41412], Loss: 2.2501, Perplexity: 9.48892\n",
      "Epoch [2/3], Step [6800/41412], Loss: 2.6620, Perplexity: 14.3245\n",
      "Epoch [2/3], Step [6900/41412], Loss: 2.2756, Perplexity: 9.73393\n",
      "Epoch [2/3], Step [7000/41412], Loss: 2.4201, Perplexity: 11.2465\n",
      "Epoch [2/3], Step [7100/41412], Loss: 2.4185, Perplexity: 11.2292\n",
      "Epoch [2/3], Step [7200/41412], Loss: 2.2818, Perplexity: 9.79480\n",
      "Epoch [2/3], Step [7300/41412], Loss: 2.2260, Perplexity: 9.26297\n",
      "Epoch [2/3], Step [7400/41412], Loss: 2.3728, Perplexity: 10.7278\n",
      "Epoch [2/3], Step [7500/41412], Loss: 2.0503, Perplexity: 7.77026\n",
      "Epoch [2/3], Step [7600/41412], Loss: 2.9316, Perplexity: 18.7569\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [7700/41412], Loss: 1.8950, Perplexity: 6.65246\n",
      "Epoch [2/3], Step [7800/41412], Loss: 2.3742, Perplexity: 10.7429\n",
      "Epoch [2/3], Step [7900/41412], Loss: 2.2319, Perplexity: 9.31745\n",
      "Epoch [2/3], Step [8000/41412], Loss: 2.1636, Perplexity: 8.70215\n",
      "Epoch [2/3], Step [8100/41412], Loss: 2.3788, Perplexity: 10.7919\n",
      "Epoch [2/3], Step [8200/41412], Loss: 1.9754, Perplexity: 7.20964\n",
      "Epoch [2/3], Step [8300/41412], Loss: 2.3030, Perplexity: 10.0037\n",
      "Epoch [2/3], Step [8400/41412], Loss: 2.6064, Perplexity: 13.5498\n",
      "Epoch [2/3], Step [8500/41412], Loss: 2.5768, Perplexity: 13.1549\n",
      "Epoch [2/3], Step [8600/41412], Loss: 2.1612, Perplexity: 8.681989\n",
      "Epoch [2/3], Step [8700/41412], Loss: 2.1128, Perplexity: 8.27138\n",
      "Epoch [2/3], Step [8800/41412], Loss: 1.9586, Perplexity: 7.08954\n",
      "Epoch [2/3], Step [8900/41412], Loss: 2.2349, Perplexity: 9.34544\n",
      "Epoch [2/3], Step [9000/41412], Loss: 2.1106, Perplexity: 8.25295\n",
      "Epoch [2/3], Step [9100/41412], Loss: 2.0041, Perplexity: 7.41969\n",
      "Epoch [2/3], Step [9200/41412], Loss: 1.9278, Perplexity: 6.87479\n",
      "Epoch [2/3], Step [9300/41412], Loss: 1.9603, Perplexity: 7.10156\n",
      "Epoch [2/3], Step [9400/41412], Loss: 2.1292, Perplexity: 8.40820\n",
      "Epoch [2/3], Step [9500/41412], Loss: 1.9845, Perplexity: 7.27528\n",
      "Epoch [2/3], Step [9600/41412], Loss: 2.0780, Perplexity: 7.98853\n",
      "Epoch [2/3], Step [9700/41412], Loss: 2.0531, Perplexity: 7.79186\n",
      "Epoch [2/3], Step [9800/41412], Loss: 2.1547, Perplexity: 8.62559\n",
      "Epoch [2/3], Step [9900/41412], Loss: 2.2144, Perplexity: 9.15620\n",
      "Epoch [2/3], Step [10000/41412], Loss: 2.4721, Perplexity: 11.8472\n",
      "Epoch [2/3], Step [10100/41412], Loss: 2.1074, Perplexity: 8.22666\n",
      "Epoch [2/3], Step [10200/41412], Loss: 2.1961, Perplexity: 8.98981\n",
      "Epoch [2/3], Step [10300/41412], Loss: 2.0759, Perplexity: 7.97204\n",
      "Epoch [2/3], Step [10400/41412], Loss: 2.2228, Perplexity: 9.23316\n",
      "Epoch [2/3], Step [10500/41412], Loss: 2.2769, Perplexity: 9.74679\n",
      "Epoch [2/3], Step [10600/41412], Loss: 2.3513, Perplexity: 10.4990\n",
      "Epoch [2/3], Step [10700/41412], Loss: 2.3193, Perplexity: 10.1681\n",
      "Epoch [2/3], Step [10800/41412], Loss: 2.2207, Perplexity: 9.21411\n",
      "Epoch [2/3], Step [10900/41412], Loss: 2.3628, Perplexity: 10.6207\n",
      "Epoch [2/3], Step [11000/41412], Loss: 2.3314, Perplexity: 10.2919\n",
      "Epoch [2/3], Step [11100/41412], Loss: 1.9094, Perplexity: 6.74895\n",
      "Epoch [2/3], Step [11200/41412], Loss: 1.9517, Perplexity: 7.04087\n",
      "Epoch [2/3], Step [11300/41412], Loss: 2.7442, Perplexity: 15.5524\n",
      "Epoch [2/3], Step [11400/41412], Loss: 2.5357, Perplexity: 12.6256\n",
      "Epoch [2/3], Step [11500/41412], Loss: 2.0593, Perplexity: 7.84054\n",
      "Epoch [2/3], Step [11600/41412], Loss: 2.9439, Perplexity: 18.9901\n",
      "Epoch [2/3], Step [11700/41412], Loss: 2.1727, Perplexity: 8.78163\n",
      "Epoch [2/3], Step [11800/41412], Loss: 2.1410, Perplexity: 8.50798\n",
      "Epoch [2/3], Step [11900/41412], Loss: 1.8282, Perplexity: 6.222963\n",
      "Epoch [2/3], Step [12000/41412], Loss: 2.4273, Perplexity: 11.3279\n",
      "Epoch [2/3], Step [12100/41412], Loss: 2.6458, Perplexity: 14.0943\n",
      "Epoch [2/3], Step [12200/41412], Loss: 2.7332, Perplexity: 15.3822\n",
      "Epoch [2/3], Step [12300/41412], Loss: 2.1793, Perplexity: 8.83999\n",
      "Epoch [2/3], Step [12400/41412], Loss: 2.0456, Perplexity: 7.73371\n",
      "Epoch [2/3], Step [12500/41412], Loss: 2.3268, Perplexity: 10.2449\n",
      "Epoch [2/3], Step [12600/41412], Loss: 2.3677, Perplexity: 10.6727\n",
      "Epoch [2/3], Step [12700/41412], Loss: 2.9243, Perplexity: 18.6217\n",
      "Epoch [2/3], Step [12800/41412], Loss: 2.1339, Perplexity: 8.44797\n",
      "Epoch [2/3], Step [12900/41412], Loss: 2.0663, Perplexity: 7.89587\n",
      "Epoch [2/3], Step [13000/41412], Loss: 2.4801, Perplexity: 11.9428\n",
      "Epoch [2/3], Step [13100/41412], Loss: 2.2500, Perplexity: 9.48769\n",
      "Epoch [2/3], Step [13200/41412], Loss: 2.3045, Perplexity: 10.0191\n",
      "Epoch [2/3], Step [13300/41412], Loss: 2.2855, Perplexity: 9.83074\n",
      "Epoch [2/3], Step [13400/41412], Loss: 2.0475, Perplexity: 7.74884\n",
      "Epoch [2/3], Step [13500/41412], Loss: 2.5136, Perplexity: 12.3494\n",
      "Epoch [2/3], Step [13600/41412], Loss: 1.7591, Perplexity: 5.80726\n",
      "Epoch [2/3], Step [13700/41412], Loss: 2.0652, Perplexity: 7.88664\n",
      "Epoch [2/3], Step [13800/41412], Loss: 2.3138, Perplexity: 10.1125\n",
      "Epoch [2/3], Step [13900/41412], Loss: 2.8599, Perplexity: 17.4598\n",
      "Epoch [2/3], Step [14000/41412], Loss: 2.0030, Perplexity: 7.41142\n",
      "Epoch [2/3], Step [14100/41412], Loss: 1.8412, Perplexity: 6.30399\n",
      "Epoch [2/3], Step [14200/41412], Loss: 2.4289, Perplexity: 11.3468\n",
      "Epoch [2/3], Step [14300/41412], Loss: 2.4706, Perplexity: 11.8293\n",
      "Epoch [2/3], Step [14400/41412], Loss: 2.4471, Perplexity: 11.5547\n",
      "Epoch [2/3], Step [14500/41412], Loss: 2.8461, Perplexity: 17.2199\n",
      "Epoch [2/3], Step [14600/41412], Loss: 2.2324, Perplexity: 9.32219\n",
      "Epoch [2/3], Step [14700/41412], Loss: 2.0608, Perplexity: 7.85222\n",
      "Epoch [2/3], Step [14800/41412], Loss: 2.1962, Perplexity: 8.99030\n",
      "Epoch [2/3], Step [14900/41412], Loss: 2.2108, Perplexity: 9.12356\n",
      "Epoch [2/3], Step [15000/41412], Loss: 2.0137, Perplexity: 7.49116\n",
      "Epoch [2/3], Step [15100/41412], Loss: 2.0106, Perplexity: 7.46788\n",
      "Epoch [2/3], Step [15200/41412], Loss: 1.8696, Perplexity: 6.48580\n",
      "Epoch [2/3], Step [15300/41412], Loss: 2.1852, Perplexity: 8.89221\n",
      "Epoch [2/3], Step [15400/41412], Loss: 2.3945, Perplexity: 10.9624\n",
      "Epoch [2/3], Step [15500/41412], Loss: 2.8236, Perplexity: 16.8377\n",
      "Epoch [2/3], Step [15600/41412], Loss: 2.2977, Perplexity: 9.95094\n",
      "Epoch [2/3], Step [15700/41412], Loss: 2.1722, Perplexity: 8.77769\n",
      "Epoch [2/3], Step [15800/41412], Loss: 2.3272, Perplexity: 10.2490\n",
      "Epoch [2/3], Step [15900/41412], Loss: 2.6083, Perplexity: 13.5755\n",
      "Epoch [2/3], Step [16000/41412], Loss: 2.1026, Perplexity: 8.18742\n",
      "Epoch [2/3], Step [16100/41412], Loss: 2.3695, Perplexity: 10.6916\n",
      "Epoch [2/3], Step [16200/41412], Loss: 2.2558, Perplexity: 9.54279\n",
      "Epoch [2/3], Step [16300/41412], Loss: 2.0633, Perplexity: 7.87179\n",
      "Epoch [2/3], Step [16400/41412], Loss: 2.2301, Perplexity: 9.30046\n",
      "Epoch [2/3], Step [16500/41412], Loss: 1.8751, Perplexity: 6.52171\n",
      "Epoch [2/3], Step [16600/41412], Loss: 2.2078, Perplexity: 9.09596\n",
      "Epoch [2/3], Step [16700/41412], Loss: 1.9535, Perplexity: 7.05358\n",
      "Epoch [2/3], Step [16800/41412], Loss: 2.3381, Perplexity: 10.3619\n",
      "Epoch [2/3], Step [16900/41412], Loss: 2.1768, Perplexity: 8.81829\n",
      "Epoch [2/3], Step [17000/41412], Loss: 2.3195, Perplexity: 10.1711\n",
      "Epoch [2/3], Step [17100/41412], Loss: 2.2270, Perplexity: 9.27246\n",
      "Epoch [2/3], Step [17200/41412], Loss: 2.0838, Perplexity: 8.03494\n",
      "Epoch [2/3], Step [17300/41412], Loss: 2.2928, Perplexity: 9.902459\n",
      "Epoch [2/3], Step [17400/41412], Loss: 1.9136, Perplexity: 6.77778\n",
      "Epoch [2/3], Step [17500/41412], Loss: 1.9731, Perplexity: 7.19310\n",
      "Epoch [2/3], Step [17600/41412], Loss: 2.7312, Perplexity: 15.3507\n",
      "Epoch [2/3], Step [17700/41412], Loss: 1.8561, Perplexity: 6.39903\n",
      "Epoch [2/3], Step [17800/41412], Loss: 2.5260, Perplexity: 12.5028\n",
      "Epoch [2/3], Step [17900/41412], Loss: 2.0762, Perplexity: 7.97422\n",
      "Epoch [2/3], Step [18000/41412], Loss: 2.9981, Perplexity: 20.0469\n",
      "Epoch [2/3], Step [18100/41412], Loss: 2.0090, Perplexity: 7.45620\n",
      "Epoch [2/3], Step [18200/41412], Loss: 2.4682, Perplexity: 11.8009\n",
      "Epoch [2/3], Step [18300/41412], Loss: 2.3169, Perplexity: 10.1440\n",
      "Epoch [2/3], Step [18400/41412], Loss: 2.5883, Perplexity: 13.3068\n",
      "Epoch [2/3], Step [18500/41412], Loss: 2.4559, Perplexity: 11.6572\n",
      "Epoch [2/3], Step [18600/41412], Loss: 2.2181, Perplexity: 9.18950\n",
      "Epoch [2/3], Step [18700/41412], Loss: 2.3352, Perplexity: 10.3317\n",
      "Epoch [2/3], Step [18800/41412], Loss: 1.9352, Perplexity: 6.925115\n",
      "Epoch [2/3], Step [18900/41412], Loss: 2.2129, Perplexity: 9.14193\n",
      "Epoch [2/3], Step [19000/41412], Loss: 2.6179, Perplexity: 13.7063\n",
      "Epoch [2/3], Step [19100/41412], Loss: 2.0614, Perplexity: 7.85690\n",
      "Epoch [2/3], Step [19200/41412], Loss: 1.8606, Perplexity: 6.42781\n",
      "Epoch [2/3], Step [19300/41412], Loss: 2.1374, Perplexity: 8.47736\n",
      "Epoch [2/3], Step [19400/41412], Loss: 2.2399, Perplexity: 9.39213\n",
      "Epoch [2/3], Step [19500/41412], Loss: 2.3611, Perplexity: 10.6024\n",
      "Epoch [2/3], Step [19600/41412], Loss: 1.8416, Perplexity: 6.30683\n",
      "Epoch [2/3], Step [19700/41412], Loss: 2.5917, Perplexity: 13.3522\n",
      "Epoch [2/3], Step [19800/41412], Loss: 2.1424, Perplexity: 8.51977\n",
      "Epoch [2/3], Step [19900/41412], Loss: 1.5619, Perplexity: 4.76775\n",
      "Epoch [2/3], Step [20000/41412], Loss: 1.8041, Perplexity: 6.07481\n",
      "Epoch [2/3], Step [20100/41412], Loss: 1.7423, Perplexity: 5.71048\n",
      "Epoch [2/3], Step [20200/41412], Loss: 2.1026, Perplexity: 8.18727\n",
      "Epoch [2/3], Step [20300/41412], Loss: 2.9457, Perplexity: 19.0243\n",
      "Epoch [2/3], Step [20400/41412], Loss: 2.1094, Perplexity: 8.24351\n",
      "Epoch [2/3], Step [20500/41412], Loss: 2.2585, Perplexity: 9.56863\n",
      "Epoch [2/3], Step [20600/41412], Loss: 1.9651, Perplexity: 7.13533\n",
      "Epoch [2/3], Step [20700/41412], Loss: 2.5297, Perplexity: 12.5493\n",
      "Epoch [2/3], Step [20800/41412], Loss: 2.8973, Perplexity: 18.1247\n",
      "Epoch [2/3], Step [20900/41412], Loss: 2.1218, Perplexity: 8.34612\n",
      "Epoch [2/3], Step [21000/41412], Loss: 2.6515, Perplexity: 14.1755\n",
      "Epoch [2/3], Step [21100/41412], Loss: 1.8132, Perplexity: 6.13002\n",
      "Epoch [2/3], Step [21200/41412], Loss: 2.5850, Perplexity: 13.2633\n",
      "Epoch [2/3], Step [21300/41412], Loss: 2.1728, Perplexity: 8.78255\n",
      "Epoch [2/3], Step [21400/41412], Loss: 2.3304, Perplexity: 10.2822\n",
      "Epoch [2/3], Step [21500/41412], Loss: 1.9000, Perplexity: 6.68594\n",
      "Epoch [2/3], Step [21600/41412], Loss: 2.3661, Perplexity: 10.6554\n",
      "Epoch [2/3], Step [21700/41412], Loss: 2.0413, Perplexity: 7.70032\n",
      "Epoch [2/3], Step [21800/41412], Loss: 2.1139, Perplexity: 8.28076\n",
      "Epoch [2/3], Step [21900/41412], Loss: 2.3478, Perplexity: 10.4627\n",
      "Epoch [2/3], Step [22000/41412], Loss: 2.0353, Perplexity: 7.65433\n",
      "Epoch [2/3], Step [22100/41412], Loss: 1.9883, Perplexity: 7.30314\n",
      "Epoch [2/3], Step [22200/41412], Loss: 2.0084, Perplexity: 7.45128\n",
      "Epoch [2/3], Step [22300/41412], Loss: 2.8363, Perplexity: 17.0523\n",
      "Epoch [2/3], Step [22400/41412], Loss: 2.3866, Perplexity: 10.8766\n",
      "Epoch [2/3], Step [22500/41412], Loss: 2.0359, Perplexity: 7.65945\n",
      "Epoch [2/3], Step [22600/41412], Loss: 2.0494, Perplexity: 7.76339\n",
      "Epoch [2/3], Step [22700/41412], Loss: 1.7948, Perplexity: 6.01853\n",
      "Epoch [2/3], Step [22800/41412], Loss: 2.8807, Perplexity: 17.8263\n",
      "Epoch [2/3], Step [22900/41412], Loss: 2.0391, Perplexity: 7.68357\n",
      "Epoch [2/3], Step [23000/41412], Loss: 2.9482, Perplexity: 19.0724\n",
      "Epoch [2/3], Step [23100/41412], Loss: 2.1518, Perplexity: 8.60069\n",
      "Epoch [2/3], Step [23200/41412], Loss: 1.9667, Perplexity: 7.14695\n",
      "Epoch [2/3], Step [23300/41412], Loss: 1.9282, Perplexity: 6.87740\n",
      "Epoch [2/3], Step [23400/41412], Loss: 1.9101, Perplexity: 6.75409\n",
      "Epoch [2/3], Step [23500/41412], Loss: 2.4566, Perplexity: 11.6646\n",
      "Epoch [2/3], Step [23600/41412], Loss: 2.0591, Perplexity: 7.83858\n",
      "Epoch [2/3], Step [23700/41412], Loss: 2.8416, Perplexity: 17.1427\n",
      "Epoch [2/3], Step [23800/41412], Loss: 2.0008, Perplexity: 7.39496\n",
      "Epoch [2/3], Step [23900/41412], Loss: 2.1325, Perplexity: 8.43582\n",
      "Epoch [2/3], Step [24000/41412], Loss: 1.8368, Perplexity: 6.27676\n",
      "Epoch [2/3], Step [24100/41412], Loss: 2.2969, Perplexity: 9.94389\n",
      "Epoch [2/3], Step [24200/41412], Loss: 2.2724, Perplexity: 9.70317\n",
      "Epoch [2/3], Step [24300/41412], Loss: 2.1498, Perplexity: 8.58285\n",
      "Epoch [2/3], Step [24400/41412], Loss: 2.0943, Perplexity: 8.11978\n",
      "Epoch [2/3], Step [24500/41412], Loss: 2.3264, Perplexity: 10.2408\n",
      "Epoch [2/3], Step [24600/41412], Loss: 2.9197, Perplexity: 18.5357\n",
      "Epoch [2/3], Step [24700/41412], Loss: 2.6903, Perplexity: 14.7356\n",
      "Epoch [2/3], Step [24800/41412], Loss: 3.0103, Perplexity: 20.2940\n",
      "Epoch [2/3], Step [24900/41412], Loss: 2.3617, Perplexity: 10.6087\n",
      "Epoch [2/3], Step [25000/41412], Loss: 1.8908, Perplexity: 6.62499\n",
      "Epoch [2/3], Step [25100/41412], Loss: 2.2598, Perplexity: 9.58104\n",
      "Epoch [2/3], Step [25200/41412], Loss: 2.8342, Perplexity: 17.0160\n",
      "Epoch [2/3], Step [25300/41412], Loss: 1.6980, Perplexity: 5.46287\n",
      "Epoch [2/3], Step [25400/41412], Loss: 2.2839, Perplexity: 9.81497\n",
      "Epoch [2/3], Step [25500/41412], Loss: 2.4505, Perplexity: 11.5945\n",
      "Epoch [2/3], Step [25600/41412], Loss: 1.9307, Perplexity: 6.89433\n",
      "Epoch [2/3], Step [25700/41412], Loss: 2.3328, Perplexity: 10.3070\n",
      "Epoch [2/3], Step [25800/41412], Loss: 1.7771, Perplexity: 5.91245\n",
      "Epoch [2/3], Step [25900/41412], Loss: 2.2292, Perplexity: 9.29228\n",
      "Epoch [2/3], Step [26000/41412], Loss: 2.5457, Perplexity: 12.7523\n",
      "Epoch [2/3], Step [26100/41412], Loss: 2.8314, Perplexity: 16.9691\n",
      "Epoch [2/3], Step [26200/41412], Loss: 2.5104, Perplexity: 12.3097\n",
      "Epoch [2/3], Step [26300/41412], Loss: 2.0251, Perplexity: 7.57708\n",
      "Epoch [2/3], Step [26400/41412], Loss: 1.7553, Perplexity: 5.78544\n",
      "Epoch [2/3], Step [26500/41412], Loss: 1.7876, Perplexity: 5.97537\n",
      "Epoch [2/3], Step [26600/41412], Loss: 2.0046, Perplexity: 7.42332\n",
      "Epoch [2/3], Step [26700/41412], Loss: 1.7199, Perplexity: 5.58394\n",
      "Epoch [2/3], Step [26800/41412], Loss: 1.9996, Perplexity: 7.38609\n",
      "Epoch [2/3], Step [26900/41412], Loss: 2.7450, Perplexity: 15.56545\n",
      "Epoch [2/3], Step [27000/41412], Loss: 2.1132, Perplexity: 8.27481\n",
      "Epoch [2/3], Step [27100/41412], Loss: 2.6092, Perplexity: 13.5881\n",
      "Epoch [2/3], Step [27200/41412], Loss: 2.0477, Perplexity: 7.74982\n",
      "Epoch [2/3], Step [27300/41412], Loss: 2.2341, Perplexity: 9.33785\n",
      "Epoch [2/3], Step [27400/41412], Loss: 1.8824, Perplexity: 6.56954\n",
      "Epoch [2/3], Step [27500/41412], Loss: 2.3850, Perplexity: 10.85911\n",
      "Epoch [2/3], Step [27600/41412], Loss: 1.9517, Perplexity: 7.04070\n",
      "Epoch [2/3], Step [27700/41412], Loss: 2.1274, Perplexity: 8.39318\n",
      "Epoch [2/3], Step [27800/41412], Loss: 2.0481, Perplexity: 7.75331\n",
      "Epoch [2/3], Step [27900/41412], Loss: 1.8947, Perplexity: 6.65084\n",
      "Epoch [2/3], Step [28000/41412], Loss: 2.1356, Perplexity: 8.46201\n",
      "Epoch [2/3], Step [28100/41412], Loss: 2.6329, Perplexity: 13.9143\n",
      "Epoch [2/3], Step [28200/41412], Loss: 2.0153, Perplexity: 7.50331\n",
      "Epoch [2/3], Step [28300/41412], Loss: 2.5958, Perplexity: 13.4074\n",
      "Epoch [2/3], Step [28400/41412], Loss: 2.0948, Perplexity: 8.12369\n",
      "Epoch [2/3], Step [28500/41412], Loss: 2.4205, Perplexity: 11.2515\n",
      "Epoch [2/3], Step [28600/41412], Loss: 2.0444, Perplexity: 7.72468\n",
      "Epoch [2/3], Step [28700/41412], Loss: 2.1144, Perplexity: 8.28424\n",
      "Epoch [2/3], Step [28800/41412], Loss: 2.4779, Perplexity: 11.9158\n",
      "Epoch [2/3], Step [28900/41412], Loss: 2.4735, Perplexity: 11.8644\n",
      "Epoch [2/3], Step [29000/41412], Loss: 2.6015, Perplexity: 13.4841\n",
      "Epoch [2/3], Step [29100/41412], Loss: 2.5772, Perplexity: 13.1603\n",
      "Epoch [2/3], Step [29200/41412], Loss: 1.8235, Perplexity: 6.19368\n",
      "Epoch [2/3], Step [29300/41412], Loss: 2.4465, Perplexity: 11.5475\n",
      "Epoch [2/3], Step [29400/41412], Loss: 2.0705, Perplexity: 7.92870\n",
      "Epoch [2/3], Step [29500/41412], Loss: 1.9266, Perplexity: 6.86637\n",
      "Epoch [2/3], Step [29600/41412], Loss: 2.5908, Perplexity: 13.3405\n",
      "Epoch [2/3], Step [29700/41412], Loss: 2.4441, Perplexity: 11.5200\n",
      "Epoch [2/3], Step [29800/41412], Loss: 2.2573, Perplexity: 9.55718\n",
      "Epoch [2/3], Step [29900/41412], Loss: 2.6116, Perplexity: 13.6212\n",
      "Epoch [2/3], Step [30000/41412], Loss: 1.9398, Perplexity: 6.95715\n",
      "Epoch [2/3], Step [30100/41412], Loss: 2.3426, Perplexity: 10.4086\n",
      "Epoch [2/3], Step [30200/41412], Loss: 2.1677, Perplexity: 8.73804\n",
      "Epoch [2/3], Step [30300/41412], Loss: 3.3726, Perplexity: 29.1536\n",
      "Epoch [2/3], Step [30400/41412], Loss: 2.5886, Perplexity: 13.3105\n",
      "Epoch [2/3], Step [30500/41412], Loss: 1.9875, Perplexity: 7.29725\n",
      "Epoch [2/3], Step [30600/41412], Loss: 2.2841, Perplexity: 9.81665\n",
      "Epoch [2/3], Step [30700/41412], Loss: 2.0771, Perplexity: 7.98098\n",
      "Epoch [2/3], Step [30800/41412], Loss: 2.3292, Perplexity: 10.2701\n",
      "Epoch [2/3], Step [30900/41412], Loss: 2.6405, Perplexity: 14.0199\n",
      "Epoch [2/3], Step [31000/41412], Loss: 2.0023, Perplexity: 7.40601\n",
      "Epoch [2/3], Step [31100/41412], Loss: 2.5175, Perplexity: 12.3970\n",
      "Epoch [2/3], Step [31200/41412], Loss: 2.3832, Perplexity: 10.8396\n",
      "Epoch [2/3], Step [31300/41412], Loss: 2.1388, Perplexity: 8.48889\n",
      "Epoch [2/3], Step [31400/41412], Loss: 2.0640, Perplexity: 7.87788\n",
      "Epoch [2/3], Step [31500/41412], Loss: 1.8043, Perplexity: 6.07592\n",
      "Epoch [2/3], Step [31600/41412], Loss: 2.0621, Perplexity: 7.86267\n",
      "Epoch [2/3], Step [31700/41412], Loss: 1.7434, Perplexity: 5.71694\n",
      "Epoch [2/3], Step [31800/41412], Loss: 2.0754, Perplexity: 7.96808\n",
      "Epoch [2/3], Step [31900/41412], Loss: 2.0440, Perplexity: 7.72138\n",
      "Epoch [2/3], Step [32000/41412], Loss: 1.7140, Perplexity: 5.55092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3], Step [32100/41412], Loss: 2.2532, Perplexity: 9.51802\n",
      "Epoch [2/3], Step [32200/41412], Loss: 2.5818, Perplexity: 13.2203\n",
      "Epoch [2/3], Step [32300/41412], Loss: 2.2541, Perplexity: 9.52635\n",
      "Epoch [2/3], Step [32400/41412], Loss: 2.2319, Perplexity: 9.31724\n",
      "Epoch [2/3], Step [32500/41412], Loss: 2.1984, Perplexity: 9.01031\n",
      "Epoch [2/3], Step [32600/41412], Loss: 1.9067, Perplexity: 6.73123\n",
      "Epoch [2/3], Step [32700/41412], Loss: 2.0320, Perplexity: 7.62963\n",
      "Epoch [2/3], Step [32800/41412], Loss: 1.9365, Perplexity: 6.93461\n",
      "Epoch [2/3], Step [32900/41412], Loss: 2.8890, Perplexity: 17.9752\n",
      "Epoch [2/3], Step [33000/41412], Loss: 2.2594, Perplexity: 9.57721\n",
      "Epoch [2/3], Step [33100/41412], Loss: 2.0086, Perplexity: 7.45261\n",
      "Epoch [2/3], Step [33200/41412], Loss: 2.3545, Perplexity: 10.5329\n",
      "Epoch [2/3], Step [33300/41412], Loss: 2.5979, Perplexity: 13.4356\n",
      "Epoch [2/3], Step [33400/41412], Loss: 2.2573, Perplexity: 9.55726\n",
      "Epoch [2/3], Step [33500/41412], Loss: 1.9177, Perplexity: 6.80527\n",
      "Epoch [2/3], Step [33600/41412], Loss: 2.4717, Perplexity: 11.8430\n",
      "Epoch [2/3], Step [33700/41412], Loss: 2.2993, Perplexity: 9.96727\n",
      "Epoch [2/3], Step [33800/41412], Loss: 2.5136, Perplexity: 12.3491\n",
      "Epoch [2/3], Step [33900/41412], Loss: 2.5480, Perplexity: 12.7811\n",
      "Epoch [2/3], Step [34000/41412], Loss: 2.3806, Perplexity: 10.8117\n",
      "Epoch [2/3], Step [34100/41412], Loss: 2.0069, Perplexity: 7.43992\n",
      "Epoch [2/3], Step [34200/41412], Loss: 2.4825, Perplexity: 11.97157\n",
      "Epoch [2/3], Step [34300/41412], Loss: 2.3574, Perplexity: 10.5637\n",
      "Epoch [2/3], Step [34400/41412], Loss: 2.4432, Perplexity: 11.5100\n",
      "Epoch [2/3], Step [34500/41412], Loss: 2.3438, Perplexity: 10.4208\n",
      "Epoch [2/3], Step [34600/41412], Loss: 2.1885, Perplexity: 8.92220\n",
      "Epoch [2/3], Step [34700/41412], Loss: 2.6295, Perplexity: 13.8675\n",
      "Epoch [2/3], Step [34800/41412], Loss: 2.0229, Perplexity: 7.55999\n",
      "Epoch [2/3], Step [34900/41412], Loss: 1.9900, Perplexity: 7.31564\n",
      "Epoch [2/3], Step [35000/41412], Loss: 2.0494, Perplexity: 7.76334\n",
      "Epoch [2/3], Step [35100/41412], Loss: 2.0483, Perplexity: 7.75444\n",
      "Epoch [2/3], Step [35200/41412], Loss: 1.9961, Perplexity: 7.36028\n",
      "Epoch [2/3], Step [35300/41412], Loss: 2.0117, Perplexity: 7.47580\n",
      "Epoch [2/3], Step [35400/41412], Loss: 2.1063, Perplexity: 8.21815\n",
      "Epoch [2/3], Step [35500/41412], Loss: 2.4487, Perplexity: 11.5736\n",
      "Epoch [2/3], Step [35600/41412], Loss: 2.0377, Perplexity: 7.67328\n",
      "Epoch [2/3], Step [35700/41412], Loss: 2.8697, Perplexity: 17.6319\n",
      "Epoch [2/3], Step [35800/41412], Loss: 2.4392, Perplexity: 11.4638\n",
      "Epoch [2/3], Step [35900/41412], Loss: 2.3167, Perplexity: 10.1424\n",
      "Epoch [2/3], Step [36000/41412], Loss: 1.9818, Perplexity: 7.25611\n",
      "Epoch [2/3], Step [36100/41412], Loss: 1.9378, Perplexity: 6.94337\n",
      "Epoch [2/3], Step [36200/41412], Loss: 1.8647, Perplexity: 6.45403\n",
      "Epoch [2/3], Step [36300/41412], Loss: 1.8747, Perplexity: 6.51902\n",
      "Epoch [2/3], Step [36400/41412], Loss: 2.6858, Perplexity: 14.6703\n",
      "Epoch [2/3], Step [36500/41412], Loss: 2.4642, Perplexity: 11.7538\n",
      "Epoch [2/3], Step [36600/41412], Loss: 2.4583, Perplexity: 11.6855\n",
      "Epoch [2/3], Step [36700/41412], Loss: 2.3331, Perplexity: 10.3099\n",
      "Epoch [2/3], Step [36800/41412], Loss: 2.2139, Perplexity: 9.15123\n",
      "Epoch [2/3], Step [36900/41412], Loss: 1.8822, Perplexity: 6.568113\n",
      "Epoch [2/3], Step [37000/41412], Loss: 2.8258, Perplexity: 16.8742\n",
      "Epoch [2/3], Step [37100/41412], Loss: 2.2608, Perplexity: 9.59064\n",
      "Epoch [2/3], Step [37200/41412], Loss: 3.0585, Perplexity: 21.2961\n",
      "Epoch [2/3], Step [37300/41412], Loss: 2.6316, Perplexity: 13.8961\n",
      "Epoch [2/3], Step [37400/41412], Loss: 2.5088, Perplexity: 12.2896\n",
      "Epoch [2/3], Step [37500/41412], Loss: 2.0727, Perplexity: 7.94605\n",
      "Epoch [2/3], Step [37600/41412], Loss: 2.2605, Perplexity: 9.58780\n",
      "Epoch [2/3], Step [37700/41412], Loss: 2.0904, Perplexity: 8.08855\n",
      "Epoch [2/3], Step [37800/41412], Loss: 2.0902, Perplexity: 8.08690\n",
      "Epoch [2/3], Step [37900/41412], Loss: 2.5311, Perplexity: 12.5667\n",
      "Epoch [2/3], Step [38000/41412], Loss: 2.5463, Perplexity: 12.7602\n",
      "Epoch [2/3], Step [38100/41412], Loss: 2.5399, Perplexity: 12.6779\n",
      "Epoch [2/3], Step [38200/41412], Loss: 1.9770, Perplexity: 7.22115\n",
      "Epoch [2/3], Step [38300/41412], Loss: 2.6471, Perplexity: 14.1125\n",
      "Epoch [2/3], Step [38400/41412], Loss: 3.0652, Perplexity: 21.4382\n",
      "Epoch [2/3], Step [38500/41412], Loss: 1.7326, Perplexity: 5.65512\n",
      "Epoch [2/3], Step [38600/41412], Loss: 2.2570, Perplexity: 9.55473\n",
      "Epoch [2/3], Step [38700/41412], Loss: 1.5324, Perplexity: 4.62944\n",
      "Epoch [2/3], Step [38800/41412], Loss: 1.9562, Perplexity: 7.07231\n",
      "Epoch [2/3], Step [38900/41412], Loss: 2.3518, Perplexity: 10.5042\n",
      "Epoch [2/3], Step [39000/41412], Loss: 1.9952, Perplexity: 7.35380\n",
      "Epoch [2/3], Step [39100/41412], Loss: 1.9469, Perplexity: 7.00693\n",
      "Epoch [2/3], Step [39200/41412], Loss: 2.5091, Perplexity: 12.2936\n",
      "Epoch [2/3], Step [39300/41412], Loss: 2.2101, Perplexity: 9.11658\n",
      "Epoch [2/3], Step [39400/41412], Loss: 2.9223, Perplexity: 18.5843\n",
      "Epoch [2/3], Step [39500/41412], Loss: 1.5302, Perplexity: 4.61920\n",
      "Epoch [2/3], Step [39600/41412], Loss: 1.8843, Perplexity: 6.58145\n",
      "Epoch [2/3], Step [39700/41412], Loss: 2.2798, Perplexity: 9.77503\n",
      "Epoch [2/3], Step [39800/41412], Loss: 2.3776, Perplexity: 10.7790\n",
      "Epoch [2/3], Step [39900/41412], Loss: 2.6175, Perplexity: 13.7017\n",
      "Epoch [2/3], Step [40000/41412], Loss: 1.9039, Perplexity: 6.71218\n",
      "Epoch [2/3], Step [40100/41412], Loss: 1.8967, Perplexity: 6.66424\n",
      "Epoch [2/3], Step [40200/41412], Loss: 2.5711, Perplexity: 13.0799\n",
      "Epoch [2/3], Step [40300/41412], Loss: 2.1477, Perplexity: 8.56495\n",
      "Epoch [2/3], Step [40400/41412], Loss: 2.3972, Perplexity: 10.9923\n",
      "Epoch [2/3], Step [40500/41412], Loss: 1.8545, Perplexity: 6.38876\n",
      "Epoch [2/3], Step [40600/41412], Loss: 2.5072, Perplexity: 12.2704\n",
      "Epoch [2/3], Step [40700/41412], Loss: 2.4841, Perplexity: 11.9909\n",
      "Epoch [2/3], Step [40800/41412], Loss: 1.8617, Perplexity: 6.43450\n",
      "Epoch [2/3], Step [40900/41412], Loss: 2.0074, Perplexity: 7.44384\n",
      "Epoch [2/3], Step [41000/41412], Loss: 1.7798, Perplexity: 5.92859\n",
      "Epoch [2/3], Step [41100/41412], Loss: 1.4637, Perplexity: 4.32189\n",
      "Epoch [2/3], Step [41200/41412], Loss: 2.1610, Perplexity: 8.67959\n",
      "Epoch [2/3], Step [41300/41412], Loss: 1.9309, Perplexity: 6.89546\n",
      "Epoch [2/3], Step [41400/41412], Loss: 3.3055, Perplexity: 27.2610\n",
      "Epoch [3/3], Step [100/41412], Loss: 2.0425, Perplexity: 7.7098747\n",
      "Epoch [3/3], Step [200/41412], Loss: 2.2225, Perplexity: 9.23072\n",
      "Epoch [3/3], Step [300/41412], Loss: 2.9866, Perplexity: 19.8172\n",
      "Epoch [3/3], Step [400/41412], Loss: 2.5298, Perplexity: 12.5506\n",
      "Epoch [3/3], Step [500/41412], Loss: 2.4309, Perplexity: 11.3694\n",
      "Epoch [3/3], Step [600/41412], Loss: 2.3713, Perplexity: 10.7111\n",
      "Epoch [3/3], Step [700/41412], Loss: 2.7149, Perplexity: 15.1031\n",
      "Epoch [3/3], Step [800/41412], Loss: 2.5592, Perplexity: 12.9257\n",
      "Epoch [3/3], Step [900/41412], Loss: 2.0308, Perplexity: 7.62021\n",
      "Epoch [3/3], Step [1000/41412], Loss: 2.1186, Perplexity: 8.3192\n",
      "Epoch [3/3], Step [1100/41412], Loss: 2.0550, Perplexity: 7.80689\n",
      "Epoch [3/3], Step [1200/41412], Loss: 2.3609, Perplexity: 10.6007\n",
      "Epoch [3/3], Step [1300/41412], Loss: 2.6929, Perplexity: 14.7740\n",
      "Epoch [3/3], Step [1400/41412], Loss: 2.4415, Perplexity: 11.4905\n",
      "Epoch [3/3], Step [1500/41412], Loss: 2.5847, Perplexity: 13.2596\n",
      "Epoch [3/3], Step [1600/41412], Loss: 2.4307, Perplexity: 11.3667\n",
      "Epoch [3/3], Step [1700/41412], Loss: 2.1371, Perplexity: 8.47460\n",
      "Epoch [3/3], Step [1800/41412], Loss: 2.3041, Perplexity: 10.0151\n",
      "Epoch [3/3], Step [1900/41412], Loss: 2.2286, Perplexity: 9.28654\n",
      "Epoch [3/3], Step [2000/41412], Loss: 2.6361, Perplexity: 13.9592\n",
      "Epoch [3/3], Step [2100/41412], Loss: 2.5140, Perplexity: 12.3537\n",
      "Epoch [3/3], Step [2200/41412], Loss: 2.5247, Perplexity: 12.4876\n",
      "Epoch [3/3], Step [2300/41412], Loss: 1.9963, Perplexity: 7.36148\n",
      "Epoch [3/3], Step [2400/41412], Loss: 2.3175, Perplexity: 10.1499\n",
      "Epoch [3/3], Step [2500/41412], Loss: 2.1013, Perplexity: 8.17651\n",
      "Epoch [3/3], Step [2600/41412], Loss: 2.3261, Perplexity: 10.2382\n",
      "Epoch [3/3], Step [2700/41412], Loss: 1.8383, Perplexity: 6.28559\n",
      "Epoch [3/3], Step [2800/41412], Loss: 2.1136, Perplexity: 8.27774\n",
      "Epoch [3/3], Step [2900/41412], Loss: 2.0507, Perplexity: 7.77330\n",
      "Epoch [3/3], Step [3000/41412], Loss: 2.0899, Perplexity: 8.08441\n",
      "Epoch [3/3], Step [3100/41412], Loss: 3.2075, Perplexity: 24.7173\n",
      "Epoch [3/3], Step [3200/41412], Loss: 2.6156, Perplexity: 13.6754\n",
      "Epoch [3/3], Step [3300/41412], Loss: 2.6085, Perplexity: 13.5790\n",
      "Epoch [3/3], Step [3400/41412], Loss: 2.7967, Perplexity: 16.3904\n",
      "Epoch [3/3], Step [3500/41412], Loss: 1.9170, Perplexity: 6.80071\n",
      "Epoch [3/3], Step [3600/41412], Loss: 2.0876, Perplexity: 8.06587\n",
      "Epoch [3/3], Step [3700/41412], Loss: 2.2168, Perplexity: 9.17807\n",
      "Epoch [3/3], Step [3800/41412], Loss: 2.7660, Perplexity: 15.8950\n",
      "Epoch [3/3], Step [3900/41412], Loss: 1.9289, Perplexity: 6.88212\n",
      "Epoch [3/3], Step [4000/41412], Loss: 2.0905, Perplexity: 8.08884\n",
      "Epoch [3/3], Step [4100/41412], Loss: 1.9830, Perplexity: 7.26427\n",
      "Epoch [3/3], Step [4200/41412], Loss: 2.1309, Perplexity: 8.42220\n",
      "Epoch [3/3], Step [4300/41412], Loss: 2.5298, Perplexity: 12.5508\n",
      "Epoch [3/3], Step [4400/41412], Loss: 2.4408, Perplexity: 11.4822\n",
      "Epoch [3/3], Step [4500/41412], Loss: 2.0267, Perplexity: 7.58922\n",
      "Epoch [3/3], Step [4600/41412], Loss: 2.5047, Perplexity: 12.2399\n",
      "Epoch [3/3], Step [4700/41412], Loss: 2.8696, Perplexity: 17.6308\n",
      "Epoch [3/3], Step [4800/41412], Loss: 1.6210, Perplexity: 5.05802\n",
      "Epoch [3/3], Step [4900/41412], Loss: 1.8688, Perplexity: 6.48050\n",
      "Epoch [3/3], Step [5000/41412], Loss: 2.3399, Perplexity: 10.3803\n",
      "Epoch [3/3], Step [5100/41412], Loss: 2.0652, Perplexity: 7.88680\n",
      "Epoch [3/3], Step [5200/41412], Loss: 2.0677, Perplexity: 7.90698\n",
      "Epoch [3/3], Step [5300/41412], Loss: 1.7788, Perplexity: 5.92261\n",
      "Epoch [3/3], Step [5400/41412], Loss: 2.1584, Perplexity: 8.65770\n",
      "Epoch [3/3], Step [5500/41412], Loss: 2.4647, Perplexity: 11.7604\n",
      "Epoch [3/3], Step [5600/41412], Loss: 2.1222, Perplexity: 8.34916\n",
      "Epoch [3/3], Step [5700/41412], Loss: 2.0894, Perplexity: 8.08018\n",
      "Epoch [3/3], Step [5800/41412], Loss: 2.0471, Perplexity: 7.74528\n",
      "Epoch [3/3], Step [5900/41412], Loss: 2.3073, Perplexity: 10.0469\n",
      "Epoch [3/3], Step [6000/41412], Loss: 1.8799, Perplexity: 6.55275\n",
      "Epoch [3/3], Step [6100/41412], Loss: 2.1716, Perplexity: 8.77258\n",
      "Epoch [3/3], Step [6200/41412], Loss: 1.9042, Perplexity: 6.71437\n",
      "Epoch [3/3], Step [6300/41412], Loss: 1.9853, Perplexity: 7.28168\n",
      "Epoch [3/3], Step [6400/41412], Loss: 2.0629, Perplexity: 7.86865\n",
      "Epoch [3/3], Step [6500/41412], Loss: 2.2897, Perplexity: 9.87246\n",
      "Epoch [3/3], Step [6600/41412], Loss: 2.2438, Perplexity: 9.42884\n",
      "Epoch [3/3], Step [6700/41412], Loss: 2.2481, Perplexity: 9.46981\n",
      "Epoch [3/3], Step [6800/41412], Loss: 1.8726, Perplexity: 6.50508\n",
      "Epoch [3/3], Step [6900/41412], Loss: 2.3368, Perplexity: 10.3481\n",
      "Epoch [3/3], Step [7000/41412], Loss: 2.0179, Perplexity: 7.52229\n",
      "Epoch [3/3], Step [7100/41412], Loss: 2.2423, Perplexity: 9.41479\n",
      "Epoch [3/3], Step [7200/41412], Loss: 1.9213, Perplexity: 6.83001\n",
      "Epoch [3/3], Step [7300/41412], Loss: 1.8880, Perplexity: 6.60653\n",
      "Epoch [3/3], Step [7400/41412], Loss: 2.1360, Perplexity: 8.46582\n",
      "Epoch [3/3], Step [7500/41412], Loss: 2.5270, Perplexity: 12.5156\n",
      "Epoch [3/3], Step [7600/41412], Loss: 2.4666, Perplexity: 11.7825\n",
      "Epoch [3/3], Step [7700/41412], Loss: 2.1176, Perplexity: 8.31124\n",
      "Epoch [3/3], Step [7800/41412], Loss: 2.0983, Perplexity: 8.15256\n",
      "Epoch [3/3], Step [7900/41412], Loss: 2.2303, Perplexity: 9.30296\n",
      "Epoch [3/3], Step [8000/41412], Loss: 2.4561, Perplexity: 11.6589\n",
      "Epoch [3/3], Step [8100/41412], Loss: 2.4810, Perplexity: 11.9529\n",
      "Epoch [3/3], Step [8200/41412], Loss: 3.1746, Perplexity: 23.9183\n",
      "Epoch [3/3], Step [8300/41412], Loss: 2.4842, Perplexity: 11.9914\n",
      "Epoch [3/3], Step [8400/41412], Loss: 2.0831, Perplexity: 8.02939\n",
      "Epoch [3/3], Step [8500/41412], Loss: 1.9534, Perplexity: 7.05292\n",
      "Epoch [3/3], Step [8600/41412], Loss: 2.0859, Perplexity: 8.05191\n",
      "Epoch [3/3], Step [8700/41412], Loss: 2.2738, Perplexity: 9.71654\n",
      "Epoch [3/3], Step [8800/41412], Loss: 2.0445, Perplexity: 7.72566\n",
      "Epoch [3/3], Step [8900/41412], Loss: 1.9370, Perplexity: 6.93760\n",
      "Epoch [3/3], Step [9000/41412], Loss: 2.1202, Perplexity: 8.33316\n",
      "Epoch [3/3], Step [9100/41412], Loss: 2.3166, Perplexity: 10.1414\n",
      "Epoch [3/3], Step [9200/41412], Loss: 1.8487, Perplexity: 6.35137\n",
      "Epoch [3/3], Step [9300/41412], Loss: 2.2147, Perplexity: 9.15843\n",
      "Epoch [3/3], Step [9400/41412], Loss: 2.3483, Perplexity: 10.4681\n",
      "Epoch [3/3], Step [9500/41412], Loss: 2.0488, Perplexity: 7.75845\n",
      "Epoch [3/3], Step [9600/41412], Loss: 2.0545, Perplexity: 7.80327\n",
      "Epoch [3/3], Step [9700/41412], Loss: 2.7229, Perplexity: 15.2241\n",
      "Epoch [3/3], Step [9800/41412], Loss: 2.0500, Perplexity: 7.768168\n",
      "Epoch [3/3], Step [9900/41412], Loss: 2.0695, Perplexity: 7.92118\n",
      "Epoch [3/3], Step [10000/41412], Loss: 2.2555, Perplexity: 9.5404\n",
      "Epoch [3/3], Step [10100/41412], Loss: 1.8483, Perplexity: 6.34937\n",
      "Epoch [3/3], Step [10200/41412], Loss: 2.7930, Perplexity: 16.3294\n",
      "Epoch [3/3], Step [10300/41412], Loss: 2.4476, Perplexity: 11.5608\n",
      "Epoch [3/3], Step [10400/41412], Loss: 2.1647, Perplexity: 8.71207\n",
      "Epoch [3/3], Step [10500/41412], Loss: 2.7833, Perplexity: 16.1731\n",
      "Epoch [3/3], Step [10600/41412], Loss: 2.2774, Perplexity: 9.75080\n",
      "Epoch [3/3], Step [10700/41412], Loss: 2.0361, Perplexity: 7.66050\n",
      "Epoch [3/3], Step [10800/41412], Loss: 2.2105, Perplexity: 9.12041\n",
      "Epoch [3/3], Step [10900/41412], Loss: 2.4870, Perplexity: 12.0249\n",
      "Epoch [3/3], Step [11000/41412], Loss: 2.3353, Perplexity: 10.3324\n",
      "Epoch [3/3], Step [11100/41412], Loss: 2.5522, Perplexity: 12.8347\n",
      "Epoch [3/3], Step [11200/41412], Loss: 1.9310, Perplexity: 6.89645\n",
      "Epoch [3/3], Step [11300/41412], Loss: 2.8080, Perplexity: 16.5767\n",
      "Epoch [3/3], Step [11400/41412], Loss: 2.0098, Perplexity: 7.46183\n",
      "Epoch [3/3], Step [11500/41412], Loss: 2.4387, Perplexity: 11.4586\n",
      "Epoch [3/3], Step [11600/41412], Loss: 2.0388, Perplexity: 7.68125\n",
      "Epoch [3/3], Step [11700/41412], Loss: 2.0061, Perplexity: 7.43460\n",
      "Epoch [3/3], Step [11800/41412], Loss: 2.8489, Perplexity: 17.2690\n",
      "Epoch [3/3], Step [11900/41412], Loss: 2.5308, Perplexity: 12.5631\n",
      "Epoch [3/3], Step [12000/41412], Loss: 2.1993, Perplexity: 9.01865\n",
      "Epoch [3/3], Step [12100/41412], Loss: 2.1042, Perplexity: 8.20088\n",
      "Epoch [3/3], Step [12200/41412], Loss: 2.3472, Perplexity: 10.4561\n",
      "Epoch [3/3], Step [12300/41412], Loss: 2.0621, Perplexity: 7.86260\n",
      "Epoch [3/3], Step [12400/41412], Loss: 2.3506, Perplexity: 10.4914\n",
      "Epoch [3/3], Step [12500/41412], Loss: 2.2214, Perplexity: 9.22019\n",
      "Epoch [3/3], Step [12600/41412], Loss: 2.2986, Perplexity: 9.96053\n",
      "Epoch [3/3], Step [12700/41412], Loss: 3.4788, Perplexity: 32.4210\n",
      "Epoch [3/3], Step [12800/41412], Loss: 2.6982, Perplexity: 14.8525\n",
      "Epoch [3/3], Step [12900/41412], Loss: 2.5433, Perplexity: 12.7214\n",
      "Epoch [3/3], Step [13000/41412], Loss: 1.6214, Perplexity: 5.06031\n",
      "Epoch [3/3], Step [13100/41412], Loss: 2.4849, Perplexity: 11.9995\n",
      "Epoch [3/3], Step [13200/41412], Loss: 2.2714, Perplexity: 9.69306\n",
      "Epoch [3/3], Step [13300/41412], Loss: 2.0724, Perplexity: 7.94356\n",
      "Epoch [3/3], Step [13400/41412], Loss: 1.8794, Perplexity: 6.54973\n",
      "Epoch [3/3], Step [13500/41412], Loss: 2.0321, Perplexity: 7.62986\n",
      "Epoch [3/3], Step [13600/41412], Loss: 2.0335, Perplexity: 7.64059\n",
      "Epoch [3/3], Step [13700/41412], Loss: 2.4893, Perplexity: 12.0526\n",
      "Epoch [3/3], Step [13800/41412], Loss: 2.2673, Perplexity: 9.65375\n",
      "Epoch [3/3], Step [13900/41412], Loss: 2.3157, Perplexity: 10.1320\n",
      "Epoch [3/3], Step [14000/41412], Loss: 2.2740, Perplexity: 9.71838\n",
      "Epoch [3/3], Step [14100/41412], Loss: 2.6362, Perplexity: 13.9594\n",
      "Epoch [3/3], Step [14200/41412], Loss: 2.4843, Perplexity: 11.9924\n",
      "Epoch [3/3], Step [14300/41412], Loss: 2.1279, Perplexity: 8.39767\n",
      "Epoch [3/3], Step [14400/41412], Loss: 2.5160, Perplexity: 12.3784\n",
      "Epoch [3/3], Step [14500/41412], Loss: 1.6935, Perplexity: 5.43844\n",
      "Epoch [3/3], Step [14600/41412], Loss: 2.1118, Perplexity: 8.26354\n",
      "Epoch [3/3], Step [14700/41412], Loss: 2.4628, Perplexity: 11.7377\n",
      "Epoch [3/3], Step [14800/41412], Loss: 2.2563, Perplexity: 9.54750\n",
      "Epoch [3/3], Step [14900/41412], Loss: 2.0526, Perplexity: 7.78805\n",
      "Epoch [3/3], Step [15000/41412], Loss: 2.6714, Perplexity: 14.4606\n",
      "Epoch [3/3], Step [15100/41412], Loss: 2.3290, Perplexity: 10.2676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [15200/41412], Loss: 2.0020, Perplexity: 7.40373\n",
      "Epoch [3/3], Step [15300/41412], Loss: 1.8491, Perplexity: 6.35397\n",
      "Epoch [3/3], Step [15400/41412], Loss: 1.9421, Perplexity: 6.973533\n",
      "Epoch [3/3], Step [15500/41412], Loss: 2.3070, Perplexity: 10.0447\n",
      "Epoch [3/3], Step [15600/41412], Loss: 1.7758, Perplexity: 5.90502\n",
      "Epoch [3/3], Step [15700/41412], Loss: 1.9844, Perplexity: 7.27492\n",
      "Epoch [3/3], Step [15800/41412], Loss: 2.4221, Perplexity: 11.2692\n",
      "Epoch [3/3], Step [15900/41412], Loss: 2.2663, Perplexity: 9.64418\n",
      "Epoch [3/3], Step [16000/41412], Loss: 2.0573, Perplexity: 7.82460\n",
      "Epoch [3/3], Step [16100/41412], Loss: 1.9994, Perplexity: 7.38480\n",
      "Epoch [3/3], Step [16200/41412], Loss: 2.0318, Perplexity: 7.62773\n",
      "Epoch [3/3], Step [16300/41412], Loss: 1.7661, Perplexity: 5.84829\n",
      "Epoch [3/3], Step [16400/41412], Loss: 1.8408, Perplexity: 6.30161\n",
      "Epoch [3/3], Step [16500/41412], Loss: 2.8287, Perplexity: 16.9234\n",
      "Epoch [3/3], Step [16600/41412], Loss: 1.5776, Perplexity: 4.84330\n",
      "Epoch [3/3], Step [16700/41412], Loss: 2.3869, Perplexity: 10.8792\n",
      "Epoch [3/3], Step [16800/41412], Loss: 2.7163, Perplexity: 15.1239\n",
      "Epoch [3/3], Step [16900/41412], Loss: 2.0368, Perplexity: 7.66605\n",
      "Epoch [3/3], Step [17000/41412], Loss: 2.0483, Perplexity: 7.75488\n",
      "Epoch [3/3], Step [17100/41412], Loss: 2.0531, Perplexity: 7.79203\n",
      "Epoch [3/3], Step [17200/41412], Loss: 2.0836, Perplexity: 8.03328\n",
      "Epoch [3/3], Step [17300/41412], Loss: 1.7404, Perplexity: 5.69971\n",
      "Epoch [3/3], Step [17400/41412], Loss: 1.9337, Perplexity: 6.91472\n",
      "Epoch [3/3], Step [17500/41412], Loss: 2.1862, Perplexity: 8.90094\n",
      "Epoch [3/3], Step [17600/41412], Loss: 2.7283, Perplexity: 15.3075\n",
      "Epoch [3/3], Step [17700/41412], Loss: 2.1896, Perplexity: 8.93147\n",
      "Epoch [3/3], Step [17800/41412], Loss: 2.0146, Perplexity: 7.49798\n",
      "Epoch [3/3], Step [17900/41412], Loss: 1.9514, Perplexity: 7.03894\n",
      "Epoch [3/3], Step [18000/41412], Loss: 2.1871, Perplexity: 8.90955\n",
      "Epoch [3/3], Step [18100/41412], Loss: 2.6971, Perplexity: 14.8366\n",
      "Epoch [3/3], Step [18200/41412], Loss: 1.9627, Perplexity: 7.11840\n",
      "Epoch [3/3], Step [18300/41412], Loss: 2.5389, Perplexity: 12.6653\n",
      "Epoch [3/3], Step [18400/41412], Loss: 1.9742, Perplexity: 7.20095\n",
      "Epoch [3/3], Step [18500/41412], Loss: 2.7212, Perplexity: 15.1982\n",
      "Epoch [3/3], Step [18600/41412], Loss: 2.6678, Perplexity: 14.4080\n",
      "Epoch [3/3], Step [18700/41412], Loss: 2.6328, Perplexity: 13.9129\n",
      "Epoch [3/3], Step [18800/41412], Loss: 2.0684, Perplexity: 7.91187\n",
      "Epoch [3/3], Step [18900/41412], Loss: 2.3265, Perplexity: 10.2420\n",
      "Epoch [3/3], Step [19000/41412], Loss: 2.3324, Perplexity: 10.3025\n",
      "Epoch [3/3], Step [19100/41412], Loss: 1.6782, Perplexity: 5.35608\n",
      "Epoch [3/3], Step [19200/41412], Loss: 2.3481, Perplexity: 10.4654\n",
      "Epoch [3/3], Step [19300/41412], Loss: 1.8897, Perplexity: 6.61778\n",
      "Epoch [3/3], Step [19400/41412], Loss: 2.4047, Perplexity: 11.0749\n",
      "Epoch [3/3], Step [19500/41412], Loss: 2.2458, Perplexity: 9.44765\n",
      "Epoch [3/3], Step [19600/41412], Loss: 2.2246, Perplexity: 9.25021\n",
      "Epoch [3/3], Step [19700/41412], Loss: 2.4491, Perplexity: 11.5784\n",
      "Epoch [3/3], Step [19800/41412], Loss: 2.1608, Perplexity: 8.67830\n",
      "Epoch [3/3], Step [19900/41412], Loss: 1.9279, Perplexity: 6.87483\n",
      "Epoch [3/3], Step [20000/41412], Loss: 2.5093, Perplexity: 12.2963\n",
      "Epoch [3/3], Step [20100/41412], Loss: 2.3190, Perplexity: 10.1659\n",
      "Epoch [3/3], Step [20200/41412], Loss: 2.3912, Perplexity: 10.9268\n",
      "Epoch [3/3], Step [20300/41412], Loss: 1.9000, Perplexity: 6.68618\n",
      "Epoch [3/3], Step [20400/41412], Loss: 2.5650, Perplexity: 13.0008\n",
      "Epoch [3/3], Step [20500/41412], Loss: 1.9940, Perplexity: 7.34481\n",
      "Epoch [3/3], Step [20600/41412], Loss: 2.5636, Perplexity: 12.9819\n",
      "Epoch [3/3], Step [20700/41412], Loss: 2.2563, Perplexity: 9.54768\n",
      "Epoch [3/3], Step [20800/41412], Loss: 2.0254, Perplexity: 7.57921\n",
      "Epoch [3/3], Step [20900/41412], Loss: 2.0080, Perplexity: 7.448355\n",
      "Epoch [3/3], Step [21000/41412], Loss: 2.5955, Perplexity: 13.4039\n",
      "Epoch [3/3], Step [21100/41412], Loss: 2.1641, Perplexity: 8.70695\n",
      "Epoch [3/3], Step [21200/41412], Loss: 2.5971, Perplexity: 13.4245\n",
      "Epoch [3/3], Step [21300/41412], Loss: 1.7988, Perplexity: 6.04228\n",
      "Epoch [3/3], Step [21400/41412], Loss: 2.2743, Perplexity: 9.72082\n",
      "Epoch [3/3], Step [21500/41412], Loss: 2.1677, Perplexity: 8.73844\n",
      "Epoch [3/3], Step [21600/41412], Loss: 2.1106, Perplexity: 8.25293\n",
      "Epoch [3/3], Step [21700/41412], Loss: 2.0285, Perplexity: 7.60232\n",
      "Epoch [3/3], Step [21800/41412], Loss: 2.6891, Perplexity: 14.7189\n",
      "Epoch [3/3], Step [21900/41412], Loss: 1.7599, Perplexity: 5.81198\n",
      "Epoch [3/3], Step [22000/41412], Loss: 2.0056, Perplexity: 7.43029\n",
      "Epoch [3/3], Step [22100/41412], Loss: 2.3079, Perplexity: 10.0532\n",
      "Epoch [3/3], Step [22200/41412], Loss: 2.0401, Perplexity: 7.69119\n",
      "Epoch [3/3], Step [22300/41412], Loss: 1.5890, Perplexity: 4.89885\n",
      "Epoch [3/3], Step [22400/41412], Loss: 2.4170, Perplexity: 11.2127\n",
      "Epoch [3/3], Step [22500/41412], Loss: 2.4856, Perplexity: 12.0077\n",
      "Epoch [3/3], Step [22600/41412], Loss: 2.4406, Perplexity: 11.4799\n",
      "Epoch [3/3], Step [22700/41412], Loss: 2.4899, Perplexity: 12.0598\n",
      "Epoch [3/3], Step [22800/41412], Loss: 1.9657, Perplexity: 7.13961\n",
      "Epoch [3/3], Step [22900/41412], Loss: 2.5056, Perplexity: 12.2505\n",
      "Epoch [3/3], Step [23000/41412], Loss: 3.0786, Perplexity: 21.7277\n",
      "Epoch [3/3], Step [23100/41412], Loss: 2.3759, Perplexity: 10.7608\n",
      "Epoch [3/3], Step [23200/41412], Loss: 1.8644, Perplexity: 6.45243\n",
      "Epoch [3/3], Step [23300/41412], Loss: 2.4275, Perplexity: 11.3310\n",
      "Epoch [3/3], Step [23400/41412], Loss: 3.4352, Perplexity: 31.0366\n",
      "Epoch [3/3], Step [23500/41412], Loss: 2.9060, Perplexity: 18.2842\n",
      "Epoch [3/3], Step [23600/41412], Loss: 1.9892, Perplexity: 7.30961\n",
      "Epoch [3/3], Step [23700/41412], Loss: 1.9849, Perplexity: 7.27846\n",
      "Epoch [3/3], Step [23800/41412], Loss: 2.1766, Perplexity: 8.81633\n",
      "Epoch [3/3], Step [23900/41412], Loss: 2.6394, Perplexity: 14.0045\n",
      "Epoch [3/3], Step [24000/41412], Loss: 2.6423, Perplexity: 14.04604\n",
      "Epoch [3/3], Step [24100/41412], Loss: 1.9612, Perplexity: 7.10821\n",
      "Epoch [3/3], Step [24200/41412], Loss: 1.9895, Perplexity: 7.31224\n",
      "Epoch [3/3], Step [24300/41412], Loss: 1.8963, Perplexity: 6.66129\n",
      "Epoch [3/3], Step [24400/41412], Loss: 2.4687, Perplexity: 11.8066\n",
      "Epoch [3/3], Step [24500/41412], Loss: 2.0344, Perplexity: 7.64769\n",
      "Epoch [3/3], Step [24600/41412], Loss: 1.8583, Perplexity: 6.41315\n",
      "Epoch [3/3], Step [24700/41412], Loss: 2.1943, Perplexity: 8.97331\n",
      "Epoch [3/3], Step [24800/41412], Loss: 2.0854, Perplexity: 8.04797\n",
      "Epoch [3/3], Step [24900/41412], Loss: 2.7253, Perplexity: 15.2604\n",
      "Epoch [3/3], Step [25000/41412], Loss: 2.7019, Perplexity: 14.9080\n",
      "Epoch [3/3], Step [25100/41412], Loss: 2.4223, Perplexity: 11.2720\n",
      "Epoch [3/3], Step [25200/41412], Loss: 2.0785, Perplexity: 7.99266\n",
      "Epoch [3/3], Step [25300/41412], Loss: 3.2561, Perplexity: 25.9469\n",
      "Epoch [3/3], Step [25400/41412], Loss: 2.2500, Perplexity: 9.48743\n",
      "Epoch [3/3], Step [25500/41412], Loss: 1.9306, Perplexity: 6.89380\n",
      "Epoch [3/3], Step [25600/41412], Loss: 2.5357, Perplexity: 12.6255\n",
      "Epoch [3/3], Step [25700/41412], Loss: 2.5325, Perplexity: 12.5854\n",
      "Epoch [3/3], Step [25800/41412], Loss: 2.5733, Perplexity: 13.1091\n",
      "Epoch [3/3], Step [25900/41412], Loss: 3.0591, Perplexity: 21.3076\n",
      "Epoch [3/3], Step [26000/41412], Loss: 2.1739, Perplexity: 8.79280\n",
      "Epoch [3/3], Step [26100/41412], Loss: 2.0212, Perplexity: 7.54737\n",
      "Epoch [3/3], Step [26200/41412], Loss: 2.4110, Perplexity: 11.1456\n",
      "Epoch [3/3], Step [26300/41412], Loss: 2.6362, Perplexity: 13.9594\n",
      "Epoch [3/3], Step [26400/41412], Loss: 2.2787, Perplexity: 9.76362\n",
      "Epoch [3/3], Step [26500/41412], Loss: 2.3509, Perplexity: 10.4952\n",
      "Epoch [3/3], Step [26600/41412], Loss: 3.1117, Perplexity: 22.4586\n",
      "Epoch [3/3], Step [26700/41412], Loss: 2.4048, Perplexity: 11.0764\n",
      "Epoch [3/3], Step [26800/41412], Loss: 1.8453, Perplexity: 6.33024\n",
      "Epoch [3/3], Step [26900/41412], Loss: 1.8946, Perplexity: 6.64986\n",
      "Epoch [3/3], Step [27000/41412], Loss: 2.4894, Perplexity: 12.0539\n",
      "Epoch [3/3], Step [27100/41412], Loss: 2.0878, Perplexity: 8.06745\n",
      "Epoch [3/3], Step [27200/41412], Loss: 1.9787, Perplexity: 7.23317\n",
      "Epoch [3/3], Step [27300/41412], Loss: 2.6340, Perplexity: 13.9295\n",
      "Epoch [3/3], Step [27400/41412], Loss: 2.0584, Perplexity: 7.83324\n",
      "Epoch [3/3], Step [27500/41412], Loss: 2.1875, Perplexity: 8.91325\n",
      "Epoch [3/3], Step [27600/41412], Loss: 2.1054, Perplexity: 8.21052\n",
      "Epoch [3/3], Step [27700/41412], Loss: 2.3080, Perplexity: 10.0546\n",
      "Epoch [3/3], Step [27800/41412], Loss: 2.2620, Perplexity: 9.60198\n",
      "Epoch [3/3], Step [27900/41412], Loss: 2.3688, Perplexity: 10.6851\n",
      "Epoch [3/3], Step [28000/41412], Loss: 1.8038, Perplexity: 6.07280\n",
      "Epoch [3/3], Step [28100/41412], Loss: 2.6400, Perplexity: 14.0127\n",
      "Epoch [3/3], Step [28200/41412], Loss: 2.1945, Perplexity: 8.97536\n",
      "Epoch [3/3], Step [28300/41412], Loss: 1.8536, Perplexity: 6.38255\n",
      "Epoch [3/3], Step [28400/41412], Loss: 2.2747, Perplexity: 9.72520\n",
      "Epoch [3/3], Step [28500/41412], Loss: 1.9566, Perplexity: 7.07525\n",
      "Epoch [3/3], Step [28600/41412], Loss: 1.8684, Perplexity: 6.47804\n",
      "Epoch [3/3], Step [28700/41412], Loss: 2.2403, Perplexity: 9.39622\n",
      "Epoch [3/3], Step [28800/41412], Loss: 1.9319, Perplexity: 6.90277\n",
      "Epoch [3/3], Step [28900/41412], Loss: 2.0627, Perplexity: 7.86721\n",
      "Epoch [3/3], Step [29000/41412], Loss: 1.8440, Perplexity: 6.32209\n",
      "Epoch [3/3], Step [29100/41412], Loss: 1.9895, Perplexity: 7.31154\n",
      "Epoch [3/3], Step [29200/41412], Loss: 1.9426, Perplexity: 6.97707\n",
      "Epoch [3/3], Step [29300/41412], Loss: 2.0377, Perplexity: 7.67303\n",
      "Epoch [3/3], Step [29400/41412], Loss: 2.5487, Perplexity: 12.7899\n",
      "Epoch [3/3], Step [29500/41412], Loss: 2.4573, Perplexity: 11.6728\n",
      "Epoch [3/3], Step [29600/41412], Loss: 2.0540, Perplexity: 7.79878\n",
      "Epoch [3/3], Step [29700/41412], Loss: 1.9283, Perplexity: 6.87799\n",
      "Epoch [3/3], Step [29800/41412], Loss: 2.1577, Perplexity: 8.65160\n",
      "Epoch [3/3], Step [29900/41412], Loss: 2.7675, Perplexity: 15.9195\n",
      "Epoch [3/3], Step [30000/41412], Loss: 2.1459, Perplexity: 8.54972\n",
      "Epoch [3/3], Step [30100/41412], Loss: 2.1604, Perplexity: 8.67480\n",
      "Epoch [3/3], Step [30200/41412], Loss: 2.3098, Perplexity: 10.0719\n",
      "Epoch [3/3], Step [30300/41412], Loss: 2.7783, Perplexity: 16.0909\n",
      "Epoch [3/3], Step [30400/41412], Loss: 2.1802, Perplexity: 8.84827\n",
      "Epoch [3/3], Step [30500/41412], Loss: 2.4833, Perplexity: 11.9812\n",
      "Epoch [3/3], Step [30600/41412], Loss: 2.0908, Perplexity: 8.09131\n",
      "Epoch [3/3], Step [30700/41412], Loss: 1.8419, Perplexity: 6.30828\n",
      "Epoch [3/3], Step [30800/41412], Loss: 2.3883, Perplexity: 10.8946\n",
      "Epoch [3/3], Step [30900/41412], Loss: 2.1703, Perplexity: 8.76102\n",
      "Epoch [3/3], Step [31000/41412], Loss: 1.9212, Perplexity: 6.82917\n",
      "Epoch [3/3], Step [31100/41412], Loss: 2.2078, Perplexity: 9.09617\n",
      "Epoch [3/3], Step [31200/41412], Loss: 1.8243, Perplexity: 6.19826\n",
      "Epoch [3/3], Step [31300/41412], Loss: 1.9034, Perplexity: 6.70864\n",
      "Epoch [3/3], Step [31400/41412], Loss: 2.7996, Perplexity: 16.4384\n",
      "Epoch [3/3], Step [31500/41412], Loss: 2.3425, Perplexity: 10.4074\n",
      "Epoch [3/3], Step [31600/41412], Loss: 3.0079, Perplexity: 20.2450\n",
      "Epoch [3/3], Step [31700/41412], Loss: 2.1634, Perplexity: 8.70049\n",
      "Epoch [3/3], Step [31800/41412], Loss: 2.2639, Perplexity: 9.62106\n",
      "Epoch [3/3], Step [31900/41412], Loss: 1.8786, Perplexity: 6.54453\n",
      "Epoch [3/3], Step [32000/41412], Loss: 2.1421, Perplexity: 8.51757\n",
      "Epoch [3/3], Step [32100/41412], Loss: 1.9720, Perplexity: 7.18517\n",
      "Epoch [3/3], Step [32200/41412], Loss: 2.2351, Perplexity: 9.34720\n",
      "Epoch [3/3], Step [32300/41412], Loss: 2.0322, Perplexity: 7.63115\n",
      "Epoch [3/3], Step [32400/41412], Loss: 2.0759, Perplexity: 7.97213\n",
      "Epoch [3/3], Step [32500/41412], Loss: 2.1752, Perplexity: 8.80447\n",
      "Epoch [3/3], Step [32600/41412], Loss: 2.0104, Perplexity: 7.46651\n",
      "Epoch [3/3], Step [32700/41412], Loss: 1.9900, Perplexity: 7.31523\n",
      "Epoch [3/3], Step [32800/41412], Loss: 2.5654, Perplexity: 13.0061\n",
      "Epoch [3/3], Step [32900/41412], Loss: 2.5220, Perplexity: 12.4531\n",
      "Epoch [3/3], Step [33000/41412], Loss: 1.6747, Perplexity: 5.33727\n",
      "Epoch [3/3], Step [33100/41412], Loss: 2.2878, Perplexity: 9.85333\n",
      "Epoch [3/3], Step [33200/41412], Loss: 2.2952, Perplexity: 9.92682\n",
      "Epoch [3/3], Step [33300/41412], Loss: 2.0046, Perplexity: 7.42283\n",
      "Epoch [3/3], Step [33400/41412], Loss: 2.1562, Perplexity: 8.63801\n",
      "Epoch [3/3], Step [33500/41412], Loss: 2.5332, Perplexity: 12.5935\n",
      "Epoch [3/3], Step [33600/41412], Loss: 2.4332, Perplexity: 11.39581\n",
      "Epoch [3/3], Step [33700/41412], Loss: 1.9692, Perplexity: 7.16528\n",
      "Epoch [3/3], Step [33800/41412], Loss: 1.7877, Perplexity: 5.97554\n",
      "Epoch [3/3], Step [33900/41412], Loss: 2.0211, Perplexity: 7.54691\n",
      "Epoch [3/3], Step [34000/41412], Loss: 2.2901, Perplexity: 9.87633\n",
      "Epoch [3/3], Step [34100/41412], Loss: 1.9135, Perplexity: 6.77711\n",
      "Epoch [3/3], Step [34200/41412], Loss: 1.9968, Perplexity: 7.36520\n",
      "Epoch [3/3], Step [34300/41412], Loss: 2.1593, Perplexity: 8.66496\n",
      "Epoch [3/3], Step [34400/41412], Loss: 2.1088, Perplexity: 8.23863\n",
      "Epoch [3/3], Step [34500/41412], Loss: 2.3451, Perplexity: 10.43409\n",
      "Epoch [3/3], Step [34600/41412], Loss: 2.7550, Perplexity: 15.7213\n",
      "Epoch [3/3], Step [34700/41412], Loss: 1.9352, Perplexity: 6.92570\n",
      "Epoch [3/3], Step [34800/41412], Loss: 1.9471, Perplexity: 7.00824\n",
      "Epoch [3/3], Step [34900/41412], Loss: 2.4662, Perplexity: 11.7779\n",
      "Epoch [3/3], Step [35000/41412], Loss: 1.6185, Perplexity: 5.04568\n",
      "Epoch [3/3], Step [35100/41412], Loss: 1.8698, Perplexity: 6.48716\n",
      "Epoch [3/3], Step [35200/41412], Loss: 2.3821, Perplexity: 10.8279\n",
      "Epoch [3/3], Step [35300/41412], Loss: 2.2219, Perplexity: 9.22514\n",
      "Epoch [3/3], Step [35400/41412], Loss: 2.4212, Perplexity: 11.2590\n",
      "Epoch [3/3], Step [35500/41412], Loss: 2.0570, Perplexity: 7.82250\n",
      "Epoch [3/3], Step [35600/41412], Loss: 2.7133, Perplexity: 15.0785\n",
      "Epoch [3/3], Step [35700/41412], Loss: 1.8665, Perplexity: 6.46578\n",
      "Epoch [3/3], Step [35800/41412], Loss: 2.2507, Perplexity: 9.49436\n",
      "Epoch [3/3], Step [35900/41412], Loss: 2.0735, Perplexity: 7.95251\n",
      "Epoch [3/3], Step [36000/41412], Loss: 2.1311, Perplexity: 8.42380\n",
      "Epoch [3/3], Step [36100/41412], Loss: 1.7145, Perplexity: 5.55425\n",
      "Epoch [3/3], Step [36200/41412], Loss: 2.1668, Perplexity: 8.73044\n",
      "Epoch [3/3], Step [36300/41412], Loss: 1.8218, Perplexity: 6.18271\n",
      "Epoch [3/3], Step [36400/41412], Loss: 2.2762, Perplexity: 9.73978\n",
      "Epoch [3/3], Step [36500/41412], Loss: 2.2533, Perplexity: 9.51943\n",
      "Epoch [3/3], Step [36600/41412], Loss: 2.4400, Perplexity: 11.4725\n",
      "Epoch [3/3], Step [36700/41412], Loss: 2.3237, Perplexity: 10.2137\n",
      "Epoch [3/3], Step [36800/41412], Loss: 2.1924, Perplexity: 8.95647\n",
      "Epoch [3/3], Step [36900/41412], Loss: 2.4459, Perplexity: 11.5414\n",
      "Epoch [3/3], Step [37000/41412], Loss: 2.0608, Perplexity: 7.85195\n",
      "Epoch [3/3], Step [37100/41412], Loss: 3.3830, Perplexity: 29.4581\n",
      "Epoch [3/3], Step [37200/41412], Loss: 2.2568, Perplexity: 9.55281\n",
      "Epoch [3/3], Step [37300/41412], Loss: 2.1626, Perplexity: 8.69416\n",
      "Epoch [3/3], Step [37400/41412], Loss: 2.1108, Perplexity: 8.25499\n",
      "Epoch [3/3], Step [37500/41412], Loss: 2.0503, Perplexity: 7.77001\n",
      "Epoch [3/3], Step [37600/41412], Loss: 1.9604, Perplexity: 7.10243\n",
      "Epoch [3/3], Step [37700/41412], Loss: 1.6151, Perplexity: 5.02826\n",
      "Epoch [3/3], Step [37800/41412], Loss: 2.2198, Perplexity: 9.20557\n",
      "Epoch [3/3], Step [37900/41412], Loss: 2.4257, Perplexity: 11.3101\n",
      "Epoch [3/3], Step [38000/41412], Loss: 1.4333, Perplexity: 4.19249\n",
      "Epoch [3/3], Step [38100/41412], Loss: 1.9748, Perplexity: 7.20522\n",
      "Epoch [3/3], Step [38200/41412], Loss: 3.0107, Perplexity: 20.3017\n",
      "Epoch [3/3], Step [38300/41412], Loss: 1.8649, Perplexity: 6.45507\n",
      "Epoch [3/3], Step [38400/41412], Loss: 2.8455, Perplexity: 17.2106\n",
      "Epoch [3/3], Step [38500/41412], Loss: 1.7176, Perplexity: 5.57142\n",
      "Epoch [3/3], Step [38600/41412], Loss: 1.9704, Perplexity: 7.17359\n",
      "Epoch [3/3], Step [38700/41412], Loss: 1.7046, Perplexity: 5.49921\n",
      "Epoch [3/3], Step [38800/41412], Loss: 1.8767, Perplexity: 6.53228\n",
      "Epoch [3/3], Step [38900/41412], Loss: 2.2818, Perplexity: 9.79420\n",
      "Epoch [3/3], Step [39000/41412], Loss: 1.8707, Perplexity: 6.49279\n",
      "Epoch [3/3], Step [39100/41412], Loss: 2.1219, Perplexity: 8.34724\n",
      "Epoch [3/3], Step [39200/41412], Loss: 2.0855, Perplexity: 8.04906\n",
      "Epoch [3/3], Step [39300/41412], Loss: 2.0348, Perplexity: 7.65049\n",
      "Epoch [3/3], Step [39400/41412], Loss: 2.0974, Perplexity: 8.14543\n",
      "Epoch [3/3], Step [39500/41412], Loss: 2.4137, Perplexity: 11.1756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3], Step [39600/41412], Loss: 1.4663, Perplexity: 4.33333\n",
      "Epoch [3/3], Step [39700/41412], Loss: 1.9306, Perplexity: 6.89392\n",
      "Epoch [3/3], Step [39800/41412], Loss: 2.8434, Perplexity: 17.1747\n",
      "Epoch [3/3], Step [39900/41412], Loss: 2.2109, Perplexity: 9.12386\n",
      "Epoch [3/3], Step [40000/41412], Loss: 2.1444, Perplexity: 8.53661\n",
      "Epoch [3/3], Step [40100/41412], Loss: 2.4855, Perplexity: 12.0069\n",
      "Epoch [3/3], Step [40200/41412], Loss: 2.8342, Perplexity: 17.0174\n",
      "Epoch [3/3], Step [40300/41412], Loss: 2.4817, Perplexity: 11.9620\n",
      "Epoch [3/3], Step [40400/41412], Loss: 2.0384, Perplexity: 7.67850\n",
      "Epoch [3/3], Step [40500/41412], Loss: 2.3387, Perplexity: 10.3681\n",
      "Epoch [3/3], Step [40600/41412], Loss: 2.2712, Perplexity: 9.69106\n",
      "Epoch [3/3], Step [40700/41412], Loss: 2.5831, Perplexity: 13.2381\n",
      "Epoch [3/3], Step [40800/41412], Loss: 2.4390, Perplexity: 11.4611\n",
      "Epoch [3/3], Step [40900/41412], Loss: 2.4320, Perplexity: 11.3819\n",
      "Epoch [3/3], Step [41000/41412], Loss: 1.7711, Perplexity: 5.87716\n",
      "Epoch [3/3], Step [41100/41412], Loss: 1.9697, Perplexity: 7.16856\n",
      "Epoch [3/3], Step [41200/41412], Loss: 2.3660, Perplexity: 10.6550\n",
      "Epoch [3/3], Step [41300/41412], Loss: 2.2724, Perplexity: 9.70260\n",
      "Epoch [3/3], Step [41400/41412], Loss: 2.7632, Perplexity: 15.8509\n",
      "Epoch [3/3], Step [41412/41412], Loss: 2.3934, Perplexity: 10.9504"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import time\n",
    "\n",
    "# Open the training log file.\n",
    "f = open(log_file, 'w')\n",
    "\n",
    "old_time = time.time()\n",
    "'''\n",
    "response = requests.request(\"GET\", \n",
    "                            \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\", \n",
    "                            headers={\"Metadata-Flavor\":\"Google\"})\n",
    "'''\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    for i_step in range(1, total_step+1):\n",
    "        '''\n",
    "        if time.time() - old_time > 60:\n",
    "            old_time = time.time()\n",
    "            requests.request(\"POST\", \n",
    "                             \"https://nebula.udacity.com/api/v1/remote/keep-alive\", \n",
    "                             headers={'Authorization': \"STAR \" + response.text})\n",
    "        '''\n",
    "        # Randomly sample a caption length, and sample indices with that length.\n",
    "        indices = data_loader.dataset.get_train_indices()\n",
    "        # Create and assign a batch sampler to retrieve a batch with the sampled indices.\n",
    "        new_sampler = data.sampler.SubsetRandomSampler(indices=indices)\n",
    "        data_loader.batch_sampler.sampler = new_sampler\n",
    "        \n",
    "        # Obtain the batch.\n",
    "        images, captions = next(iter(data_loader))\n",
    "\n",
    "        # Move batch of images and captions to GPU if CUDA is available.\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "        \n",
    "        # Zero the gradients.\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "        \n",
    "        # Pass the inputs through the CNN-RNN model.\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions)\n",
    "        \n",
    "        # Calculate the batch loss.\n",
    "        loss = criterion(outputs.view(-1, vocab_size), captions.view(-1))\n",
    "        \n",
    "        # Backward pass.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the parameters in the optimizer.\n",
    "        optimizer.step()\n",
    "            \n",
    "        # Get training statistics.\n",
    "        stats = 'Epoch [%d/%d], Step [%d/%d], Loss: %.4f, Perplexity: %5.4f' % (epoch, num_epochs, i_step, total_step, loss.item(), np.exp(loss.item()))\n",
    "        \n",
    "        # Print training statistics (on same line).\n",
    "        print('\\r' + stats, end=\"\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Print training statistics to file.\n",
    "        f.write(stats + '\\n')\n",
    "        f.flush()\n",
    "        \n",
    "        # Print training statistics (on different line).\n",
    "        if i_step % print_every == 0:\n",
    "            print('\\r' + stats)\n",
    "            \n",
    "    # Save the weights.\n",
    "    if epoch % save_every == 0:\n",
    "        torch.save(decoder.state_dict(), os.path.join('./models', 'decoder-%d.pkl' % epoch))\n",
    "        torch.save(encoder.state_dict(), os.path.join('./models', 'encoder-%d.pkl' % epoch))\n",
    "\n",
    "# Close the training log file.\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='step3'></a>\n",
    "## Step 3: (Optional) Validate your Model\n",
    "\n",
    "To assess potential overfitting, one approach is to assess performance on a validation set.  If you decide to do this **optional** task, you are required to first complete all of the steps in the next notebook in the sequence (**3_Inference.ipynb**); as part of that notebook, you will write and test code (specifically, the `sample` method in the `DecoderRNN` class) that uses your RNN decoder to generate captions.  That code will prove incredibly useful here. \n",
    "\n",
    "If you decide to validate your model, please do not edit the data loader in **data_loader.py**.  Instead, create a new file named **data_loader_val.py** containing the code for obtaining the data loader for the validation data.  You can access:\n",
    "- the validation images at filepath `'/opt/cocoapi/images/train2014/'`, and\n",
    "- the validation image caption annotation file at filepath `'/opt/cocoapi/annotations/captions_val2014.json'`.\n",
    "\n",
    "The suggested approach to validating your model involves creating a json file such as [this one](https://github.com/cocodataset/cocoapi/blob/master/results/captions_val2014_fakecap_results.json) containing your model's predicted captions for the validation images.  Then, you can write your own script or use one that you [find online](https://github.com/tylin/coco-caption) to calculate the BLEU score of your model.  You can read more about the BLEU score, along with other evaluation metrics (such as TEOR and Cider) in section 4.1 of [this paper](https://arxiv.org/pdf/1411.4555.pdf).  For more information about how to use the annotation file, check out the [website](http://cocodataset.org/#download) for the COCO dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) TODO: Validate your model."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
